{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ce9c7e",
   "metadata": {},
   "source": [
    "# ConNER: Text Preprocessing & Output Processing\n",
    "\n",
    "This notebook prepares a pipeline that (i) preprocesses an incoming text (abstract) and updates it to the format expected by ConNER; and (ii) processed the output of the model to extract the entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07d4a3",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ae36bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model definition related\n",
    "import os\n",
    "import bs4\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff019a9",
   "metadata": {},
   "source": [
    "## 2. Loading the data\n",
    "- Expects an incoming text with only the fields of \"title\" and \"abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49aab9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tricuspid valve regurgitation and lithium carbonate toxicity in a newborn infant. A newborn with massive tricuspid regurgitation, atrial flutter, congestive heart failure, and a high serum lithium level is described. This is the first patient to initially manifest tricuspid regurgitation and atrial flutter, and the 11th described patient with cardiac disease among infants exposed to lithium compounds in the first trimester of pregnancy. Sixty-three percent of these infants had tricuspid valve involvement. Lithium carbonate may be a factor in the increasing incidence of congenital heart disease when taken during early pregnancy. It also causes neurologic depression, cyanosis, and cardiac arrhythmia when consumed prior to delivery.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loads the processed CSV files from Nomita and Woojae.\n",
    "## If we want to use data from PubMed database direct, we will have to build a processing pipeline for that.\n",
    "train_path = \"./data/OfficialTrainingSet1.csv\"\n",
    "test_path = \"./data/OfficialTestSet1.csv\"\n",
    "val_path = \"./data/OfficialValidationSet1.csv\"\n",
    "\n",
    "# Reading the files but only retaining the title and abstract columns\n",
    "df_train = pd.read_csv(train_path)[['title', 'abstract']]\n",
    "df_val = pd.read_csv(test_path)[['title', 'abstract']]\n",
    "df_test = pd.read_csv(val_path)[['title', 'abstract']]\n",
    "\n",
    "# Forming a new column with the merged texts\n",
    "df_train['text'] = df_train[\"title\"] + \" \" + df_train[\"abstract\"]\n",
    "df_val['text'] = df_val[\"title\"] + \" \" + df_val[\"abstract\"]\n",
    "df_test['text'] = df_test[\"title\"] + \" \" + df_test[\"abstract\"]\n",
    "\n",
    "# This will be the starting point for further preprocessing.\n",
    "df_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3216a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_ConNER_format(df, tokenizer):\n",
    "    '''\n",
    "    - Takes in a dataframe and returns an \"example\" object that can be taken by the\n",
    "      \"load_and_cache_examples\" function from the data_utils.py function from the ConNER repo.\n",
    "    - Also outputs a mapping dictionary linking the word indices to token indices\n",
    "    \n",
    "    Inputs:\n",
    "    - df: dataframe with a \"text\" column that contains the paragraph combining the title and abstract of a journal\n",
    "    - the same tokenizer to be used in the NER model.\n",
    "    \n",
    "    Output:\n",
    "    - example object derived from the InputExample function\n",
    "    - mapping dictionary linking words to token indices\n",
    "    \n",
    "    Other Prerequisites:\n",
    "    - InputExample functionm, imported from the data_utils.py module\n",
    "    '''\n",
    "    from data_utils import InputExample\n",
    "    import copy\n",
    "    \n",
    "    mode = \"doc_dev\"  ## just inherited from the ConNER codes, stands for document-based evaluation for the dev set.\n",
    "    \n",
    "    texts = df['text']\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    mapping_dict = []\n",
    "      \n",
    "    for text in texts:\n",
    "        \n",
    "        example_dict = {} # mapping info for the current example (i.e. \"text\") only.  \n",
    "        \n",
    "        # handling the words.  For handling punctuation, referenced:\n",
    "        # https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        words =  text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation})).split()\n",
    "        labels = [0] * len(words)  ## just set labels to 0 as a dummy as we are doing inference\n",
    "        \n",
    "        token_index = 1   # starts from 1 because token 0 is the CLS token for Bert and Roberta\n",
    "        word_index = 0    # counts from 0, as words in the Scibert model is also counted from 0\n",
    "        \n",
    "        for word in words:\n",
    "            # doing a test tokenization to see the length\n",
    "            tokenized_word = tokenizer(word)['input_ids'][1:-1]   # skips the 1st & last input ids as these are the CLS and SEP tokens\n",
    "            num_of_tokens = len(tokenized_word)      # this is the no. of sub-word tokens that the current word has\n",
    "        \n",
    "            assert len(example_dict) == word_index, \"Error in code, word index is probably wrong\"\n",
    "        \n",
    "            # append a sub-list showing the list of corresponding token ids\n",
    "            token_indices = list(range(token_index, token_index + num_of_tokens))\n",
    "            example_dict[word_index] = {'word': word, 'token_idx': token_indices}\n",
    "            \n",
    "            word_index += 1\n",
    "            token_index += num_of_tokens\n",
    "            \n",
    "        mapping_dict.append(example_dict)\n",
    "        \n",
    "        if \"tags_hp\" in labels:\n",
    "            hp_labels = item[\"tags_hp\"]\n",
    "        else:\n",
    "            hp_labels = [None]*len(labels)\n",
    "                \n",
    "        examples.append(InputExample(guid=\"%s-%d\".format(mode, guid_index),\n",
    "                                     words=words,\n",
    "                                     labels=labels,\n",
    "                                     hp_labels=hp_labels))\n",
    "        guid_index += 1\n",
    "        \n",
    "    return examples, mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b3a87ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<data_utils.InputExample at 0x21ebd18a190>,\n",
       " <data_utils.InputExample at 0x21ebd18a160>,\n",
       " <data_utils.InputExample at 0x21ebd18abe0>,\n",
       " <data_utils.InputExample at 0x21ebd18acd0>,\n",
       " <data_utils.InputExample at 0x21ebd18ae80>,\n",
       " <data_utils.InputExample at 0x21ebd18a9a0>,\n",
       " <data_utils.InputExample at 0x21ebd18a520>,\n",
       " <data_utils.InputExample at 0x21ec2beb6a0>,\n",
       " <data_utils.InputExample at 0x21ec2beb9a0>,\n",
       " <data_utils.InputExample at 0x21ec2beb880>,\n",
       " <data_utils.InputExample at 0x21ec2beb730>,\n",
       " <data_utils.InputExample at 0x21ec2beb520>,\n",
       " <data_utils.InputExample at 0x21ec2bebb20>,\n",
       " <data_utils.InputExample at 0x21ec2beb7f0>,\n",
       " <data_utils.InputExample at 0x21ec2bebd30>,\n",
       " <data_utils.InputExample at 0x21d0b8ae490>,\n",
       " <data_utils.InputExample at 0x21d0b8ae3d0>,\n",
       " <data_utils.InputExample at 0x21d0b8aee50>,\n",
       " <data_utils.InputExample at 0x21d0b8ae610>,\n",
       " <data_utils.InputExample at 0x21d0b8ae850>,\n",
       " <data_utils.InputExample at 0x21d0b8ae700>,\n",
       " <data_utils.InputExample at 0x21d0b8ae370>,\n",
       " <data_utils.InputExample at 0x21d0b8ae160>,\n",
       " <data_utils.InputExample at 0x21d0b8ae9d0>,\n",
       " <data_utils.InputExample at 0x21d0b8ae8e0>,\n",
       " <data_utils.InputExample at 0x21d0b8ae9a0>,\n",
       " <data_utils.InputExample at 0x21d0b8aebb0>,\n",
       " <data_utils.InputExample at 0x21d0b8a1160>,\n",
       " <data_utils.InputExample at 0x21d0b8aee80>,\n",
       " <data_utils.InputExample at 0x21d0b8a1040>,\n",
       " <data_utils.InputExample at 0x21ebd9e10d0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1d90>,\n",
       " <data_utils.InputExample at 0x21ebd9e1be0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1550>,\n",
       " <data_utils.InputExample at 0x21ebd9e1d30>,\n",
       " <data_utils.InputExample at 0x21ebd9e1b50>,\n",
       " <data_utils.InputExample at 0x21ebd9e13d0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1ca0>,\n",
       " <data_utils.InputExample at 0x21ebd9e19d0>,\n",
       " <data_utils.InputExample at 0x21ebd9e17f0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1850>,\n",
       " <data_utils.InputExample at 0x21ebd9e1730>,\n",
       " <data_utils.InputExample at 0x21ebd9e1760>,\n",
       " <data_utils.InputExample at 0x21ebd9e1eb0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1670>,\n",
       " <data_utils.InputExample at 0x21ebd9e1fa0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1b20>,\n",
       " <data_utils.InputExample at 0x21ebd9e18b0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1ee0>,\n",
       " <data_utils.InputExample at 0x21ebd9e1190>,\n",
       " <data_utils.InputExample at 0x21ebd9e1f70>,\n",
       " <data_utils.InputExample at 0x21ebd9d06d0>,\n",
       " <data_utils.InputExample at 0x21ebd9d0fd0>,\n",
       " <data_utils.InputExample at 0x21ebd9d04c0>,\n",
       " <data_utils.InputExample at 0x21ebd9d01c0>,\n",
       " <data_utils.InputExample at 0x21ebd9d0f10>,\n",
       " <data_utils.InputExample at 0x21ebd9d0820>,\n",
       " <data_utils.InputExample at 0x21ebd9d0d30>,\n",
       " <data_utils.InputExample at 0x21ebd9d0ee0>,\n",
       " <data_utils.InputExample at 0x21ebd9d0040>,\n",
       " <data_utils.InputExample at 0x21ebd9d0400>,\n",
       " <data_utils.InputExample at 0x21ebd9d0b20>,\n",
       " <data_utils.InputExample at 0x21d0b5dd790>,\n",
       " <data_utils.InputExample at 0x21ebd9d08b0>,\n",
       " <data_utils.InputExample at 0x21ebd9d0520>,\n",
       " <data_utils.InputExample at 0x21d0b5c3310>,\n",
       " <data_utils.InputExample at 0x21eb671a250>,\n",
       " <data_utils.InputExample at 0x21eb6716340>,\n",
       " <data_utils.InputExample at 0x21d0b8ae4f0>,\n",
       " <data_utils.InputExample at 0x21d0b8ae280>,\n",
       " <data_utils.InputExample at 0x21ebd9e1df0>,\n",
       " <data_utils.InputExample at 0x21ebd18aaf0>,\n",
       " <data_utils.InputExample at 0x21ebd18a0a0>,\n",
       " <data_utils.InputExample at 0x21d267579a0>,\n",
       " <data_utils.InputExample at 0x21d26757160>,\n",
       " <data_utils.InputExample at 0x21ebd1053a0>,\n",
       " <data_utils.InputExample at 0x21ebd18a790>,\n",
       " <data_utils.InputExample at 0x21ebd18a610>,\n",
       " <data_utils.InputExample at 0x21ebd0f3d30>,\n",
       " <data_utils.InputExample at 0x21ebd0f3e20>,\n",
       " <data_utils.InputExample at 0x21d0b5dd490>,\n",
       " <data_utils.InputExample at 0x21d0b5dd370>,\n",
       " <data_utils.InputExample at 0x21d0b5dd280>,\n",
       " <data_utils.InputExample at 0x21d0b5dd730>,\n",
       " <data_utils.InputExample at 0x21d0b5dda90>,\n",
       " <data_utils.InputExample at 0x21d0b8a1e20>,\n",
       " <data_utils.InputExample at 0x21d0b8a1d00>,\n",
       " <data_utils.InputExample at 0x21d0b8a1340>,\n",
       " <data_utils.InputExample at 0x21d0b8a1580>,\n",
       " <data_utils.InputExample at 0x21d0b8a1940>,\n",
       " <data_utils.InputExample at 0x21d0b8a1460>,\n",
       " <data_utils.InputExample at 0x21d0b8a1f10>,\n",
       " <data_utils.InputExample at 0x21d0b8a1430>,\n",
       " <data_utils.InputExample at 0x21d0b8a1dc0>,\n",
       " <data_utils.InputExample at 0x21d0b8a1100>,\n",
       " <data_utils.InputExample at 0x21d0b8a1550>,\n",
       " <data_utils.InputExample at 0x21d0b8a1790>,\n",
       " <data_utils.InputExample at 0x21d0b8a16a0>,\n",
       " <data_utils.InputExample at 0x21d0b8a1670>,\n",
       " <data_utils.InputExample at 0x21d0b8a1a00>,\n",
       " <data_utils.InputExample at 0x21d0b8a17f0>,\n",
       " <data_utils.InputExample at 0x21d0b8a1280>,\n",
       " <data_utils.InputExample at 0x21d0b8a1730>,\n",
       " <data_utils.InputExample at 0x21d0b8a16d0>,\n",
       " <data_utils.InputExample at 0x21d0b8a1b50>,\n",
       " <data_utils.InputExample at 0x21ebe1d0970>,\n",
       " <data_utils.InputExample at 0x21ebe1d0a90>,\n",
       " <data_utils.InputExample at 0x21ebe1d00d0>,\n",
       " <data_utils.InputExample at 0x21ebe1d07f0>,\n",
       " <data_utils.InputExample at 0x21ebe1d0bb0>,\n",
       " <data_utils.InputExample at 0x21ebe1d0700>,\n",
       " <data_utils.InputExample at 0x21ebd8aadf0>,\n",
       " <data_utils.InputExample at 0x21ebd8aa550>,\n",
       " <data_utils.InputExample at 0x21ebd8aa880>,\n",
       " <data_utils.InputExample at 0x21ebd8aad00>,\n",
       " <data_utils.InputExample at 0x21ebd8aa730>,\n",
       " <data_utils.InputExample at 0x21ebd8aa640>,\n",
       " <data_utils.InputExample at 0x21ebd8aa8b0>,\n",
       " <data_utils.InputExample at 0x21ebd8aa700>,\n",
       " <data_utils.InputExample at 0x21ebd8aad60>,\n",
       " <data_utils.InputExample at 0x21ebd8aa910>,\n",
       " <data_utils.InputExample at 0x21ebd8aa970>,\n",
       " <data_utils.InputExample at 0x21ebd8aa430>,\n",
       " <data_utils.InputExample at 0x21ebd8aa220>,\n",
       " <data_utils.InputExample at 0x21ebd8aa9d0>,\n",
       " <data_utils.InputExample at 0x21ebd8aa190>,\n",
       " <data_utils.InputExample at 0x21ebd8aab50>,\n",
       " <data_utils.InputExample at 0x21ebd8aa280>,\n",
       " <data_utils.InputExample at 0x21ebd8aa670>,\n",
       " <data_utils.InputExample at 0x21ebd8aa310>,\n",
       " <data_utils.InputExample at 0x21d0b5af250>,\n",
       " <data_utils.InputExample at 0x21d0b5afb20>,\n",
       " <data_utils.InputExample at 0x21d0b5afdf0>,\n",
       " <data_utils.InputExample at 0x21d0b5afa90>,\n",
       " <data_utils.InputExample at 0x21d0b5aff70>,\n",
       " <data_utils.InputExample at 0x21d0b5af820>,\n",
       " <data_utils.InputExample at 0x21d0b5afd30>,\n",
       " <data_utils.InputExample at 0x21d0b5aff40>,\n",
       " <data_utils.InputExample at 0x21d0b5af790>,\n",
       " <data_utils.InputExample at 0x21d0b5af5e0>,\n",
       " <data_utils.InputExample at 0x21d0b5affa0>,\n",
       " <data_utils.InputExample at 0x21d0b5af430>,\n",
       " <data_utils.InputExample at 0x21d0b5af520>,\n",
       " <data_utils.InputExample at 0x21d0b5af220>,\n",
       " <data_utils.InputExample at 0x21d0c639520>,\n",
       " <data_utils.InputExample at 0x21d0c6396a0>,\n",
       " <data_utils.InputExample at 0x21d0b5af100>,\n",
       " <data_utils.InputExample at 0x21d0b5af400>,\n",
       " <data_utils.InputExample at 0x21d0b5af340>,\n",
       " <data_utils.InputExample at 0x21d0b5afe20>,\n",
       " <data_utils.InputExample at 0x21d0b5af370>,\n",
       " <data_utils.InputExample at 0x21d0c657280>,\n",
       " <data_utils.InputExample at 0x21d0c6578b0>,\n",
       " <data_utils.InputExample at 0x21d0c657b20>,\n",
       " <data_utils.InputExample at 0x21d0c657610>,\n",
       " <data_utils.InputExample at 0x21d0b8b29d0>,\n",
       " <data_utils.InputExample at 0x21d0b8b2040>,\n",
       " <data_utils.InputExample at 0x21d0b8b2340>,\n",
       " <data_utils.InputExample at 0x21d0b8b2940>,\n",
       " <data_utils.InputExample at 0x21d0b8b2b50>,\n",
       " <data_utils.InputExample at 0x21d0b8b2790>,\n",
       " <data_utils.InputExample at 0x21d0b8b2220>,\n",
       " <data_utils.InputExample at 0x21d0b8b20d0>,\n",
       " <data_utils.InputExample at 0x21d0b8b2e80>,\n",
       " <data_utils.InputExample at 0x21d0b8b2af0>,\n",
       " <data_utils.InputExample at 0x21d0b8b2cd0>,\n",
       " <data_utils.InputExample at 0x21d0b8b29a0>,\n",
       " <data_utils.InputExample at 0x21d0b8b24c0>,\n",
       " <data_utils.InputExample at 0x21d0b8b26a0>,\n",
       " <data_utils.InputExample at 0x21d0b8b2580>,\n",
       " <data_utils.InputExample at 0x21d0b8b2a90>,\n",
       " <data_utils.InputExample at 0x21d0b8b2b20>,\n",
       " <data_utils.InputExample at 0x21d0b8b2a60>,\n",
       " <data_utils.InputExample at 0x21d0b5ce4c0>,\n",
       " <data_utils.InputExample at 0x21d0b5cecd0>,\n",
       " <data_utils.InputExample at 0x21d0b5ce640>,\n",
       " <data_utils.InputExample at 0x21d0b5cee50>,\n",
       " <data_utils.InputExample at 0x21d0b5ce7f0>,\n",
       " <data_utils.InputExample at 0x21d0b5ce8b0>,\n",
       " <data_utils.InputExample at 0x21d0b5ceb80>,\n",
       " <data_utils.InputExample at 0x21d0b5ce1f0>,\n",
       " <data_utils.InputExample at 0x21d0b5ce220>,\n",
       " <data_utils.InputExample at 0x21d0b5ce940>,\n",
       " <data_utils.InputExample at 0x21d0b5cea30>,\n",
       " <data_utils.InputExample at 0x21d0b5ce3a0>,\n",
       " <data_utils.InputExample at 0x21d0b5ce130>,\n",
       " <data_utils.InputExample at 0x21d0b5cea60>,\n",
       " <data_utils.InputExample at 0x21d0b5ce970>,\n",
       " <data_utils.InputExample at 0x21d0b5cec10>,\n",
       " <data_utils.InputExample at 0x21d0b5cec70>,\n",
       " <data_utils.InputExample at 0x21ebd8add00>,\n",
       " <data_utils.InputExample at 0x21ebd8ad400>,\n",
       " <data_utils.InputExample at 0x21ebd8adb20>,\n",
       " <data_utils.InputExample at 0x21ebd8ade50>,\n",
       " <data_utils.InputExample at 0x21ebd8ad0a0>,\n",
       " <data_utils.InputExample at 0x21ebd8ad640>,\n",
       " <data_utils.InputExample at 0x21ebd8ad0d0>,\n",
       " <data_utils.InputExample at 0x21ebd8ade20>,\n",
       " <data_utils.InputExample at 0x21ebd8add90>,\n",
       " <data_utils.InputExample at 0x21ebd8adb50>,\n",
       " <data_utils.InputExample at 0x21ebd8ad100>,\n",
       " <data_utils.InputExample at 0x21ebd8adac0>,\n",
       " <data_utils.InputExample at 0x21ebd8adbb0>,\n",
       " <data_utils.InputExample at 0x21ebd8ad9d0>,\n",
       " <data_utils.InputExample at 0x21ebd8ada30>,\n",
       " <data_utils.InputExample at 0x21ebd8ade80>,\n",
       " <data_utils.InputExample at 0x21ebd8ad3d0>,\n",
       " <data_utils.InputExample at 0x21ebd8ad370>,\n",
       " <data_utils.InputExample at 0x21ebd8ad4c0>,\n",
       " <data_utils.InputExample at 0x21ebd8ad430>,\n",
       " <data_utils.InputExample at 0x21ebd8ad460>,\n",
       " <data_utils.InputExample at 0x21ebd8ad1f0>,\n",
       " <data_utils.InputExample at 0x21d0b5ca040>,\n",
       " <data_utils.InputExample at 0x21d0b5ca280>,\n",
       " <data_utils.InputExample at 0x21d0b5ca0a0>,\n",
       " <data_utils.InputExample at 0x21d0b5ca550>,\n",
       " <data_utils.InputExample at 0x21d0b5ca310>,\n",
       " <data_utils.InputExample at 0x21d0b5ca910>,\n",
       " <data_utils.InputExample at 0x21d0b5ca2b0>,\n",
       " <data_utils.InputExample at 0x21d0b5ca1c0>,\n",
       " <data_utils.InputExample at 0x21d0b5caaf0>,\n",
       " <data_utils.InputExample at 0x21d0b5ca7c0>,\n",
       " <data_utils.InputExample at 0x21d0b5ca9d0>,\n",
       " <data_utils.InputExample at 0x21d0b5cacd0>,\n",
       " <data_utils.InputExample at 0x21d0b5cac70>,\n",
       " <data_utils.InputExample at 0x21d0b5cad00>,\n",
       " <data_utils.InputExample at 0x21d0b5cac10>,\n",
       " <data_utils.InputExample at 0x21d0b5cadf0>,\n",
       " <data_utils.InputExample at 0x21d0b5ca9a0>,\n",
       " <data_utils.InputExample at 0x21d0b5cab80>,\n",
       " <data_utils.InputExample at 0x21d0b5cabe0>,\n",
       " <data_utils.InputExample at 0x21ebe3923a0>,\n",
       " <data_utils.InputExample at 0x21ebe3921f0>,\n",
       " <data_utils.InputExample at 0x21ebe3924c0>,\n",
       " <data_utils.InputExample at 0x21ebe392580>,\n",
       " <data_utils.InputExample at 0x21ebe392400>,\n",
       " <data_utils.InputExample at 0x21ebe392310>,\n",
       " <data_utils.InputExample at 0x21ebe3925b0>,\n",
       " <data_utils.InputExample at 0x21ebe392610>,\n",
       " <data_utils.InputExample at 0x21ebe3923d0>,\n",
       " <data_utils.InputExample at 0x21ebe392850>,\n",
       " <data_utils.InputExample at 0x21ebe3927c0>,\n",
       " <data_utils.InputExample at 0x21ebe392af0>,\n",
       " <data_utils.InputExample at 0x21ebe3921c0>,\n",
       " <data_utils.InputExample at 0x21ebe392ee0>,\n",
       " <data_utils.InputExample at 0x21ebe3929d0>,\n",
       " <data_utils.InputExample at 0x21ebe392fa0>,\n",
       " <data_utils.InputExample at 0x21ebe392760>,\n",
       " <data_utils.InputExample at 0x21ebe392d60>,\n",
       " <data_utils.InputExample at 0x21ebe392e20>,\n",
       " <data_utils.InputExample at 0x21ebe392c10>,\n",
       " <data_utils.InputExample at 0x21ebe7a8040>,\n",
       " <data_utils.InputExample at 0x21ebe7a8190>,\n",
       " <data_utils.InputExample at 0x21ebe7a8670>,\n",
       " <data_utils.InputExample at 0x21ebe7a84f0>,\n",
       " <data_utils.InputExample at 0x21ebe7a8700>,\n",
       " <data_utils.InputExample at 0x21ebe7a85b0>,\n",
       " <data_utils.InputExample at 0x21ebe7a8250>,\n",
       " <data_utils.InputExample at 0x21ebe7a8730>,\n",
       " <data_utils.InputExample at 0x21ebe7a8400>,\n",
       " <data_utils.InputExample at 0x21ebe7a8a00>,\n",
       " <data_utils.InputExample at 0x21ebe7a8d00>,\n",
       " <data_utils.InputExample at 0x21ebe7a8b80>,\n",
       " <data_utils.InputExample at 0x21ebe7a8e50>,\n",
       " <data_utils.InputExample at 0x21ebe7a8e80>,\n",
       " <data_utils.InputExample at 0x21ebe7a8df0>,\n",
       " <data_utils.InputExample at 0x21ebe7a8850>,\n",
       " <data_utils.InputExample at 0x21ebe7a8ca0>,\n",
       " <data_utils.InputExample at 0x21ebe7a8be0>,\n",
       " <data_utils.InputExample at 0x21ebf6aa2b0>,\n",
       " <data_utils.InputExample at 0x21ebf6aa310>,\n",
       " <data_utils.InputExample at 0x21ebf6aa340>,\n",
       " <data_utils.InputExample at 0x21ebf6aa250>,\n",
       " <data_utils.InputExample at 0x21ebf6aa160>,\n",
       " <data_utils.InputExample at 0x21ebf6aa4f0>,\n",
       " <data_utils.InputExample at 0x21ebf6aa700>,\n",
       " <data_utils.InputExample at 0x21ebf6aa1f0>,\n",
       " <data_utils.InputExample at 0x21ebf6aa790>,\n",
       " <data_utils.InputExample at 0x21ebf6aa820>,\n",
       " <data_utils.InputExample at 0x21ebf6aa640>,\n",
       " <data_utils.InputExample at 0x21ebf6aa550>,\n",
       " <data_utils.InputExample at 0x21ebf6aaa00>,\n",
       " <data_utils.InputExample at 0x21ebf6aa940>,\n",
       " <data_utils.InputExample at 0x21ebf6aa760>,\n",
       " <data_utils.InputExample at 0x21ebf6aaa90>,\n",
       " <data_utils.InputExample at 0x21ebf6aaac0>,\n",
       " <data_utils.InputExample at 0x21ebf6aacd0>,\n",
       " <data_utils.InputExample at 0x21ebf6aad00>,\n",
       " <data_utils.InputExample at 0x21ebf6aa850>,\n",
       " <data_utils.InputExample at 0x21ebf6aac40>,\n",
       " <data_utils.InputExample at 0x21ebf6aa7c0>,\n",
       " <data_utils.InputExample at 0x21ebf6aafa0>,\n",
       " <data_utils.InputExample at 0x21ebf6aaa60>,\n",
       " <data_utils.InputExample at 0x21ebeabe310>,\n",
       " <data_utils.InputExample at 0x21ebeabe370>,\n",
       " <data_utils.InputExample at 0x21ebeabe0a0>,\n",
       " <data_utils.InputExample at 0x21ebeabe5b0>,\n",
       " <data_utils.InputExample at 0x21ebeabe670>,\n",
       " <data_utils.InputExample at 0x21ebeabe6a0>,\n",
       " <data_utils.InputExample at 0x21ebeabe880>,\n",
       " <data_utils.InputExample at 0x21ebeabeb50>,\n",
       " <data_utils.InputExample at 0x21ebeabeaf0>,\n",
       " <data_utils.InputExample at 0x21ebeabe9a0>,\n",
       " <data_utils.InputExample at 0x21ebeabecd0>,\n",
       " <data_utils.InputExample at 0x21ebeabed00>,\n",
       " <data_utils.InputExample at 0x21ebeabe790>,\n",
       " <data_utils.InputExample at 0x21ebeabef10>,\n",
       " <data_utils.InputExample at 0x21ebeabed60>,\n",
       " <data_utils.InputExample at 0x21ebeabef70>,\n",
       " <data_utils.InputExample at 0x21ebeabeb20>,\n",
       " <data_utils.InputExample at 0x21ebf477130>,\n",
       " <data_utils.InputExample at 0x21ebf477370>,\n",
       " <data_utils.InputExample at 0x21ebf4770d0>,\n",
       " <data_utils.InputExample at 0x21ebf4770a0>,\n",
       " <data_utils.InputExample at 0x21ebf477730>,\n",
       " <data_utils.InputExample at 0x21ebf477340>,\n",
       " <data_utils.InputExample at 0x21ebf477670>,\n",
       " <data_utils.InputExample at 0x21ebf477970>,\n",
       " <data_utils.InputExample at 0x21ebf477b20>,\n",
       " <data_utils.InputExample at 0x21ebf477a90>,\n",
       " <data_utils.InputExample at 0x21ebf477dc0>,\n",
       " <data_utils.InputExample at 0x21ebf477940>,\n",
       " <data_utils.InputExample at 0x21ebf477d30>,\n",
       " <data_utils.InputExample at 0x21ebf477f40>,\n",
       " <data_utils.InputExample at 0x21ebf477b80>,\n",
       " <data_utils.InputExample at 0x21ebf477c10>,\n",
       " <data_utils.InputExample at 0x21ebe861340>,\n",
       " <data_utils.InputExample at 0x21ebe8611f0>,\n",
       " <data_utils.InputExample at 0x21ebe8615b0>,\n",
       " <data_utils.InputExample at 0x21ebe8616d0>,\n",
       " <data_utils.InputExample at 0x21ebe861370>,\n",
       " <data_utils.InputExample at 0x21ebe861730>,\n",
       " <data_utils.InputExample at 0x21ebe861580>,\n",
       " <data_utils.InputExample at 0x21ebe861a30>,\n",
       " <data_utils.InputExample at 0x21ebe861b50>,\n",
       " <data_utils.InputExample at 0x21ebe861bb0>,\n",
       " <data_utils.InputExample at 0x21ebe861c10>,\n",
       " <data_utils.InputExample at 0x21ebe861d30>,\n",
       " <data_utils.InputExample at 0x21ebe861fa0>,\n",
       " <data_utils.InputExample at 0x21ebe861fd0>,\n",
       " <data_utils.InputExample at 0x21ebe861c40>,\n",
       " <data_utils.InputExample at 0x21ebe861e80>,\n",
       " <data_utils.InputExample at 0x21ebe861e20>,\n",
       " <data_utils.InputExample at 0x21ebe861e50>,\n",
       " <data_utils.InputExample at 0x21ebf67d310>,\n",
       " <data_utils.InputExample at 0x21ebf67d2e0>,\n",
       " <data_utils.InputExample at 0x21ebf67d460>,\n",
       " <data_utils.InputExample at 0x21ebf67d340>,\n",
       " <data_utils.InputExample at 0x21ebf67d130>,\n",
       " <data_utils.InputExample at 0x21ebf67d580>,\n",
       " <data_utils.InputExample at 0x21ebf67d3d0>,\n",
       " <data_utils.InputExample at 0x21ebf67d550>,\n",
       " <data_utils.InputExample at 0x21ebf67d4f0>,\n",
       " <data_utils.InputExample at 0x21ebf67d970>,\n",
       " <data_utils.InputExample at 0x21ebf67d6d0>,\n",
       " <data_utils.InputExample at 0x21ebf67dca0>,\n",
       " <data_utils.InputExample at 0x21ebf67dd30>,\n",
       " <data_utils.InputExample at 0x21ebf67dc40>,\n",
       " <data_utils.InputExample at 0x21ebf67d820>,\n",
       " <data_utils.InputExample at 0x21ebf67d7c0>,\n",
       " <data_utils.InputExample at 0x21ebf67dfd0>,\n",
       " <data_utils.InputExample at 0x21ebf67d9a0>,\n",
       " <data_utils.InputExample at 0x21ebf67df70>,\n",
       " <data_utils.InputExample at 0x21ebf67dbb0>,\n",
       " <data_utils.InputExample at 0x21ebf4d1340>,\n",
       " <data_utils.InputExample at 0x21ebf4d10a0>,\n",
       " <data_utils.InputExample at 0x21ebf4d1130>,\n",
       " <data_utils.InputExample at 0x21ebf4d1160>,\n",
       " <data_utils.InputExample at 0x21ebf4d1250>,\n",
       " <data_utils.InputExample at 0x21ebf4d18e0>,\n",
       " <data_utils.InputExample at 0x21ebf4d1790>,\n",
       " <data_utils.InputExample at 0x21ebf4d16a0>,\n",
       " <data_utils.InputExample at 0x21ebf4d1a90>,\n",
       " <data_utils.InputExample at 0x21ebf4d1af0>,\n",
       " <data_utils.InputExample at 0x21ebf4d1700>,\n",
       " <data_utils.InputExample at 0x21ebf4d1be0>,\n",
       " <data_utils.InputExample at 0x21ebf4d1880>,\n",
       " <data_utils.InputExample at 0x21ebf4d19a0>,\n",
       " <data_utils.InputExample at 0x21ebf4d17f0>,\n",
       " <data_utils.InputExample at 0x21ebf4d1d60>,\n",
       " <data_utils.InputExample at 0x21ebf4d1a60>,\n",
       " <data_utils.InputExample at 0x21ebf4d1c70>,\n",
       " <data_utils.InputExample at 0x21ebf4d1fd0>,\n",
       " <data_utils.InputExample at 0x21ebf4d12e0>,\n",
       " <data_utils.InputExample at 0x21ec2980400>,\n",
       " <data_utils.InputExample at 0x21ec2980040>,\n",
       " <data_utils.InputExample at 0x21ec2980370>,\n",
       " <data_utils.InputExample at 0x21ec2980220>,\n",
       " <data_utils.InputExample at 0x21ec2980130>,\n",
       " <data_utils.InputExample at 0x21ec2980490>,\n",
       " <data_utils.InputExample at 0x21ec2980730>,\n",
       " <data_utils.InputExample at 0x21ec2980250>,\n",
       " <data_utils.InputExample at 0x21ec29801c0>,\n",
       " <data_utils.InputExample at 0x21ec2980790>,\n",
       " <data_utils.InputExample at 0x21ec2980ca0>,\n",
       " <data_utils.InputExample at 0x21ec2980af0>,\n",
       " <data_utils.InputExample at 0x21ec2980d60>,\n",
       " <data_utils.InputExample at 0x21ec2980dc0>,\n",
       " <data_utils.InputExample at 0x21ec2980f40>,\n",
       " <data_utils.InputExample at 0x21ec2980eb0>,\n",
       " <data_utils.InputExample at 0x21ec2980e50>,\n",
       " <data_utils.InputExample at 0x21ec2980ee0>,\n",
       " <data_utils.InputExample at 0x21ec2980a60>,\n",
       " <data_utils.InputExample at 0x21ec2ad7250>,\n",
       " <data_utils.InputExample at 0x21ec2ad7400>,\n",
       " <data_utils.InputExample at 0x21ec2ad73a0>,\n",
       " <data_utils.InputExample at 0x21ec2ad74f0>,\n",
       " <data_utils.InputExample at 0x21ec2ad7490>,\n",
       " <data_utils.InputExample at 0x21ec2ad75e0>,\n",
       " <data_utils.InputExample at 0x21ec2ad7850>,\n",
       " <data_utils.InputExample at 0x21ec2ad76a0>,\n",
       " <data_utils.InputExample at 0x21ec2ad7640>,\n",
       " <data_utils.InputExample at 0x21ec2ad7790>,\n",
       " <data_utils.InputExample at 0x21ec2ad7970>,\n",
       " <data_utils.InputExample at 0x21ec2ad79a0>,\n",
       " <data_utils.InputExample at 0x21ec2ad7df0>,\n",
       " <data_utils.InputExample at 0x21ec2ad7eb0>,\n",
       " <data_utils.InputExample at 0x21ec2ad7cd0>,\n",
       " <data_utils.InputExample at 0x21ec2e36130>,\n",
       " <data_utils.InputExample at 0x21ec2e36400>,\n",
       " <data_utils.InputExample at 0x21ec2e362b0>,\n",
       " <data_utils.InputExample at 0x21ec2e36370>,\n",
       " <data_utils.InputExample at 0x21ec2e361c0>,\n",
       " <data_utils.InputExample at 0x21ec2e36700>,\n",
       " <data_utils.InputExample at 0x21ec2e36790>,\n",
       " <data_utils.InputExample at 0x21ec2e36430>,\n",
       " <data_utils.InputExample at 0x21ec2e36a00>,\n",
       " <data_utils.InputExample at 0x21ec2e36b80>,\n",
       " <data_utils.InputExample at 0x21ec2e369d0>,\n",
       " <data_utils.InputExample at 0x21ec2e36760>,\n",
       " <data_utils.InputExample at 0x21ec2e36280>,\n",
       " <data_utils.InputExample at 0x21ec2e36e50>,\n",
       " <data_utils.InputExample at 0x21ec2e36cd0>,\n",
       " <data_utils.InputExample at 0x21ec2e36f10>,\n",
       " <data_utils.InputExample at 0x21ec2e36fd0>,\n",
       " <data_utils.InputExample at 0x21ec2e36f40>,\n",
       " <data_utils.InputExample at 0x21ec2e36ac0>,\n",
       " <data_utils.InputExample at 0x21ec2fa3280>,\n",
       " <data_utils.InputExample at 0x21ec2fa34c0>,\n",
       " <data_utils.InputExample at 0x21ec2fa36a0>,\n",
       " <data_utils.InputExample at 0x21ec2fa3670>,\n",
       " <data_utils.InputExample at 0x21ec2fa3040>,\n",
       " <data_utils.InputExample at 0x21ec2fa35e0>,\n",
       " <data_utils.InputExample at 0x21ec2fa35b0>,\n",
       " <data_utils.InputExample at 0x21ec2fa3a00>,\n",
       " <data_utils.InputExample at 0x21ec2fa3a30>,\n",
       " <data_utils.InputExample at 0x21ec2fa3af0>,\n",
       " <data_utils.InputExample at 0x21ec2fa3b20>,\n",
       " <data_utils.InputExample at 0x21ec2fa3b50>,\n",
       " <data_utils.InputExample at 0x21ec2fa3cd0>,\n",
       " <data_utils.InputExample at 0x21ec2fa3910>,\n",
       " <data_utils.InputExample at 0x21ec2fa3e50>,\n",
       " <data_utils.InputExample at 0x21ec2fa3fa0>,\n",
       " <data_utils.InputExample at 0x21ec30b2130>,\n",
       " <data_utils.InputExample at 0x21ec30b2190>,\n",
       " <data_utils.InputExample at 0x21ec30b2430>,\n",
       " <data_utils.InputExample at 0x21ec30b2460>,\n",
       " <data_utils.InputExample at 0x21ec30b24c0>,\n",
       " <data_utils.InputExample at 0x21ec30b2670>,\n",
       " <data_utils.InputExample at 0x21ec30b23a0>,\n",
       " <data_utils.InputExample at 0x21ec30b2400>,\n",
       " <data_utils.InputExample at 0x21ec30b2ac0>,\n",
       " <data_utils.InputExample at 0x21ec30b2bb0>,\n",
       " <data_utils.InputExample at 0x21ec30b2cd0>,\n",
       " <data_utils.InputExample at 0x21ec30b2d30>,\n",
       " <data_utils.InputExample at 0x21ec30b2c70>,\n",
       " <data_utils.InputExample at 0x21ec30b2940>,\n",
       " <data_utils.InputExample at 0x21ec30b2fa0>,\n",
       " <data_utils.InputExample at 0x21ec30b2fd0>,\n",
       " <data_utils.InputExample at 0x21ec30b22e0>,\n",
       " <data_utils.InputExample at 0x21ec31e4130>,\n",
       " <data_utils.InputExample at 0x21ec31e41c0>,\n",
       " <data_utils.InputExample at 0x21ec31e42b0>,\n",
       " <data_utils.InputExample at 0x21ec31e40a0>,\n",
       " <data_utils.InputExample at 0x21ec31e4580>,\n",
       " <data_utils.InputExample at 0x21ec31e4730>,\n",
       " <data_utils.InputExample at 0x21ec31e4880>,\n",
       " <data_utils.InputExample at 0x21ec31e46a0>,\n",
       " <data_utils.InputExample at 0x21ec31e48e0>,\n",
       " <data_utils.InputExample at 0x21ec31e4910>,\n",
       " <data_utils.InputExample at 0x21ec31e4a30>,\n",
       " <data_utils.InputExample at 0x21ec31e4b80>,\n",
       " <data_utils.InputExample at 0x21ec31e4be0>,\n",
       " <data_utils.InputExample at 0x21ec31e47f0>,\n",
       " <data_utils.InputExample at 0x21ec31e4c40>,\n",
       " <data_utils.InputExample at 0x21ec31e4d00>,\n",
       " <data_utils.InputExample at 0x21ec31e4bb0>,\n",
       " <data_utils.InputExample at 0x21ec31e4250>,\n",
       " <data_utils.InputExample at 0x21ec31e4f70>,\n",
       " <data_utils.InputExample at 0x21ec31e4dc0>,\n",
       " <data_utils.InputExample at 0x21ec31e4a00>,\n",
       " <data_utils.InputExample at 0x21ec31e4ac0>,\n",
       " <data_utils.InputExample at 0x21ec31e4ca0>,\n",
       " <data_utils.InputExample at 0x21ec3357250>,\n",
       " <data_utils.InputExample at 0x21ec33570a0>,\n",
       " <data_utils.InputExample at 0x21ec3357400>,\n",
       " <data_utils.InputExample at 0x21ec3357190>,\n",
       " <data_utils.InputExample at 0x21ec33572e0>,\n",
       " <data_utils.InputExample at 0x21ec33576a0>,\n",
       " <data_utils.InputExample at 0x21ec3357490>]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./ConNER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "test_set, word_to_token_map = convert_text_to_ConNER_format(df_test, tokenizer)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bf4288d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'word': 'Tricuspid', 'token_idx': [1, 2, 3]},\n",
       " 1: {'word': 'valve', 'token_idx': [4, 5]},\n",
       " 2: {'word': 'regurgitation', 'token_idx': [6, 7, 8]},\n",
       " 3: {'word': 'and', 'token_idx': [9]},\n",
       " 4: {'word': 'lithium', 'token_idx': [10, 11]},\n",
       " 5: {'word': 'carbonate', 'token_idx': [12, 13]},\n",
       " 6: {'word': 'toxicity', 'token_idx': [14]},\n",
       " 7: {'word': 'in', 'token_idx': [15]},\n",
       " 8: {'word': 'a', 'token_idx': [16]},\n",
       " 9: {'word': 'newborn', 'token_idx': [17, 18]},\n",
       " 10: {'word': 'infant.', 'token_idx': [19, 20]},\n",
       " 11: {'word': 'A', 'token_idx': [21]},\n",
       " 12: {'word': 'newborn', 'token_idx': [22, 23]},\n",
       " 13: {'word': 'with', 'token_idx': [24]},\n",
       " 14: {'word': 'massive', 'token_idx': [25, 26]},\n",
       " 15: {'word': 'tricuspid', 'token_idx': [27, 28, 29]},\n",
       " 16: {'word': 'regurgitation,', 'token_idx': [30, 31, 32, 33]},\n",
       " 17: {'word': 'atrial', 'token_idx': [34]},\n",
       " 18: {'word': 'flutter,', 'token_idx': [35, 36, 37]},\n",
       " 19: {'word': 'congestive', 'token_idx': [38, 39]},\n",
       " 20: {'word': 'heart', 'token_idx': [40]},\n",
       " 21: {'word': 'failure,', 'token_idx': [41, 42, 43]},\n",
       " 22: {'word': 'and', 'token_idx': [44]},\n",
       " 23: {'word': 'a', 'token_idx': [45]},\n",
       " 24: {'word': 'high', 'token_idx': [46]},\n",
       " 25: {'word': 'serum', 'token_idx': [47]},\n",
       " 26: {'word': 'lithium', 'token_idx': [48, 49]},\n",
       " 27: {'word': 'level', 'token_idx': [50]},\n",
       " 28: {'word': 'is', 'token_idx': [51]},\n",
       " 29: {'word': 'described.', 'token_idx': [52, 53]},\n",
       " 30: {'word': 'This', 'token_idx': [54]},\n",
       " 31: {'word': 'is', 'token_idx': [55]},\n",
       " 32: {'word': 'the', 'token_idx': [56]},\n",
       " 33: {'word': 'first', 'token_idx': [57]},\n",
       " 34: {'word': 'patient', 'token_idx': [58]},\n",
       " 35: {'word': 'to', 'token_idx': [59]},\n",
       " 36: {'word': 'initially', 'token_idx': [60, 61]},\n",
       " 37: {'word': 'manifest', 'token_idx': [62, 63]},\n",
       " 38: {'word': 'tricuspid', 'token_idx': [64, 65, 66]},\n",
       " 39: {'word': 'regurgitation', 'token_idx': [67, 68, 69]},\n",
       " 40: {'word': 'and', 'token_idx': [70]},\n",
       " 41: {'word': 'atrial', 'token_idx': [71]},\n",
       " 42: {'word': 'flutter,', 'token_idx': [72, 73, 74]},\n",
       " 43: {'word': 'and', 'token_idx': [75]},\n",
       " 44: {'word': 'the', 'token_idx': [76]},\n",
       " 45: {'word': '11th', 'token_idx': [77, 78]},\n",
       " 46: {'word': 'described', 'token_idx': [79]},\n",
       " 47: {'word': 'patient', 'token_idx': [80]},\n",
       " 48: {'word': 'with', 'token_idx': [81]},\n",
       " 49: {'word': 'cardiac', 'token_idx': [82]},\n",
       " 50: {'word': 'disease', 'token_idx': [83]},\n",
       " 51: {'word': 'among', 'token_idx': [84]},\n",
       " 52: {'word': 'infants', 'token_idx': [85, 86]},\n",
       " 53: {'word': 'exposed', 'token_idx': [87]},\n",
       " 54: {'word': 'to', 'token_idx': [88]},\n",
       " 55: {'word': 'lithium', 'token_idx': [89, 90]},\n",
       " 56: {'word': 'compounds', 'token_idx': [91, 92]},\n",
       " 57: {'word': 'in', 'token_idx': [93]},\n",
       " 58: {'word': 'the', 'token_idx': [94]},\n",
       " 59: {'word': 'first', 'token_idx': [95]},\n",
       " 60: {'word': 'trimester', 'token_idx': [96]},\n",
       " 61: {'word': 'of', 'token_idx': [97]},\n",
       " 62: {'word': 'pregnancy.', 'token_idx': [98, 99]},\n",
       " 63: {'word': 'Sixty-three', 'token_idx': [100, 101, 102]},\n",
       " 64: {'word': 'percent', 'token_idx': [103]},\n",
       " 65: {'word': 'of', 'token_idx': [104]},\n",
       " 66: {'word': 'these', 'token_idx': [105]},\n",
       " 67: {'word': 'infants', 'token_idx': [106, 107]},\n",
       " 68: {'word': 'had', 'token_idx': [108]},\n",
       " 69: {'word': 'tricuspid', 'token_idx': [109, 110, 111]},\n",
       " 70: {'word': 'valve', 'token_idx': [112, 113]},\n",
       " 71: {'word': 'involvement.', 'token_idx': [114, 115, 116]},\n",
       " 72: {'word': 'Lithium', 'token_idx': [117, 118]},\n",
       " 73: {'word': 'carbonate', 'token_idx': [119, 120]},\n",
       " 74: {'word': 'may', 'token_idx': [121]},\n",
       " 75: {'word': 'be', 'token_idx': [122]},\n",
       " 76: {'word': 'a', 'token_idx': [123]},\n",
       " 77: {'word': 'factor', 'token_idx': [124]},\n",
       " 78: {'word': 'in', 'token_idx': [125]},\n",
       " 79: {'word': 'the', 'token_idx': [126]},\n",
       " 80: {'word': 'increasing', 'token_idx': [127]},\n",
       " 81: {'word': 'incidence', 'token_idx': [128]},\n",
       " 82: {'word': 'of', 'token_idx': [129]},\n",
       " 83: {'word': 'congenital', 'token_idx': [130, 131]},\n",
       " 84: {'word': 'heart', 'token_idx': [132]},\n",
       " 85: {'word': 'disease', 'token_idx': [133]},\n",
       " 86: {'word': 'when', 'token_idx': [134]},\n",
       " 87: {'word': 'taken', 'token_idx': [135, 136]},\n",
       " 88: {'word': 'during', 'token_idx': [137]},\n",
       " 89: {'word': 'early', 'token_idx': [138]},\n",
       " 90: {'word': 'pregnancy.', 'token_idx': [139, 140]},\n",
       " 91: {'word': 'It', 'token_idx': [141]},\n",
       " 92: {'word': 'also', 'token_idx': [142]},\n",
       " 93: {'word': 'causes', 'token_idx': [143, 144]},\n",
       " 94: {'word': 'neurologic', 'token_idx': [145, 146]},\n",
       " 95: {'word': 'depression,', 'token_idx': [147, 148]},\n",
       " 96: {'word': 'cyanosis,', 'token_idx': [149, 150, 151]},\n",
       " 97: {'word': 'and', 'token_idx': [152]},\n",
       " 98: {'word': 'cardiac', 'token_idx': [153]},\n",
       " 99: {'word': 'arrhythmia', 'token_idx': [154, 155]},\n",
       " 100: {'word': 'when', 'token_idx': [156]},\n",
       " 101: {'word': 'consumed', 'token_idx': [157, 158]},\n",
       " 102: {'word': 'prior', 'token_idx': [159]},\n",
       " 103: {'word': 'to', 'token_idx': [160]},\n",
       " 104: {'word': 'delivery.', 'token_idx': [161, 162]}}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(word_to_token_map[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b2e84",
   "metadata": {},
   "source": [
    "## 3. Converting the Dataset Format & Loading Model\n",
    "- Built based on the load_and_cache_examples from the data_utils.py from the ConNER repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf26d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel,BertForTokenClassification, BertModel, RobertaModel, RobertaTokenizer, BertPreTrainedModel, RobertaConfig\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from  torch.nn.utils.rnn  import pack_padded_sequence\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss, KLDivLoss\n",
    "\n",
    "from transformers import BertConfig, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8270a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eval related\n",
    "import argparse\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Remember to copy the \"data_utils.py\" file from ConNER's repo\n",
    "from data_utils import tag_to_id, get_chunks, get_labels, convert_examples_to_features\n",
    "from flashtool import Logger\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97ae8df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ConNER_model_definition import RobertaForTokenClassification_v2\n",
    "\n",
    "## Loading model\n",
    "model_path = \"./ConNER\"\n",
    "\n",
    "## It appears the checkpoint is a Roberta-based model as loading it using BERT model yields an error.\n",
    "#test_model  = BERTForTokenClassification_v2.from_pretrained(model_path)\n",
    "\n",
    "test_model = RobertaForTokenClassification_v2.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3ce38c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, df, tokenizer, labels, pad_token_label_id, mode,\n",
    "                            entity_name='bc5cdr', remove_labels=False):\n",
    "    \n",
    "    examples, word_to_token_map = convert_text_to_ConNER_format(df, tokenizer)\n",
    "    features = convert_examples_to_features(\n",
    "        examples,\n",
    "        labels,\n",
    "        args.max_seq_length,\n",
    "        tokenizer,\n",
    "        cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "        # xlnet has a cls token at the end\n",
    "        cls_token=tokenizer.cls_token,\n",
    "        cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "        sep_token_extra=bool(args.model_type in [\"roberta\"]),\n",
    "        # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "        pad_on_left=bool(args.model_type in [\"xlnet\"]),\n",
    "        # pad on the left for xlnet\n",
    "        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "        pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
    "        pad_token_label_id=pad_token_label_id,\n",
    "        entity_name=entity_name,\n",
    "    )\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_full_label_ids = torch.tensor([f.full_label_ids for f in features], dtype=torch.long)\n",
    "    all_hp_label_ids = torch.tensor([f.hp_label_ids for f in features], dtype=torch.long)\n",
    "    all_entity_ids = torch.tensor([f.entity_ids for f in features], dtype=torch.long)\n",
    "    if remove_labels:\n",
    "        all_full_label_ids.fill_(pad_token_label_id)\n",
    "        all_hp_label_ids.fill_(pad_token_label_id)\n",
    "    all_ids = torch.tensor([f for f in range(len(features))], dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_full_label_ids, all_hp_label_ids, all_entity_ids, all_ids)\n",
    "    \n",
    "    return dataset, word_to_token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ec72c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "labels = ['O', 'B-Chemical', 'B-Disease', 'I-Chemical', 'I-Disease']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.model_type = \"roberta\"\n",
    "args.model_name_or_path = \"./ConNER\"\n",
    "args.max_seq_length = 512   ## modified from 128, ## Using 768 leads to errors!\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 8\n",
    "args.n_gpu = 1\n",
    "args.device = device\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "args.local_rank = -1\n",
    "\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 0.0\n",
    "args.adam_epsilon = 1e-8\n",
    "args.adam_beta1 = 0.9\n",
    "args.adam_beta2 = 0.98\n",
    "args.max_grad_norm = 1.0\n",
    "args.num_train_epochs = 3.0\n",
    "args.max_steps = -1\n",
    "args.warmup_steps = 0\n",
    "args.logging_steps = 10000\n",
    "args.save_steps = 10000\n",
    "args.seed = 1\n",
    "\n",
    "\n",
    "eval_dataset, word_to_token_map = load_and_cache_examples(args, df_test, tokenizer, labels, pad_token_label_id, mode=\"doc_dev\")\n",
    "eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fa6656e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'word': 'Tricuspid', 'token_idx': [1, 2, 3]},\n",
       " 1: {'word': 'valve', 'token_idx': [4, 5]},\n",
       " 2: {'word': 'regurgitation', 'token_idx': [6, 7, 8]},\n",
       " 3: {'word': 'and', 'token_idx': [9]},\n",
       " 4: {'word': 'lithium', 'token_idx': [10, 11]},\n",
       " 5: {'word': 'carbonate', 'token_idx': [12, 13]},\n",
       " 6: {'word': 'toxicity', 'token_idx': [14]},\n",
       " 7: {'word': 'in', 'token_idx': [15]},\n",
       " 8: {'word': 'a', 'token_idx': [16]},\n",
       " 9: {'word': 'newborn', 'token_idx': [17, 18]},\n",
       " 10: {'word': 'infant', 'token_idx': [19]},\n",
       " 11: {'word': '.', 'token_idx': [20]},\n",
       " 12: {'word': 'A', 'token_idx': [21]},\n",
       " 13: {'word': 'newborn', 'token_idx': [22, 23]},\n",
       " 14: {'word': 'with', 'token_idx': [24]},\n",
       " 15: {'word': 'massive', 'token_idx': [25, 26]},\n",
       " 16: {'word': 'tricuspid', 'token_idx': [27, 28, 29]},\n",
       " 17: {'word': 'regurgitation', 'token_idx': [30, 31, 32]},\n",
       " 18: {'word': ',', 'token_idx': [33]},\n",
       " 19: {'word': 'atrial', 'token_idx': [34]},\n",
       " 20: {'word': 'flutter', 'token_idx': [35, 36]},\n",
       " 21: {'word': ',', 'token_idx': [37]},\n",
       " 22: {'word': 'congestive', 'token_idx': [38, 39]},\n",
       " 23: {'word': 'heart', 'token_idx': [40]},\n",
       " 24: {'word': 'failure', 'token_idx': [41, 42]},\n",
       " 25: {'word': ',', 'token_idx': [43]},\n",
       " 26: {'word': 'and', 'token_idx': [44]},\n",
       " 27: {'word': 'a', 'token_idx': [45]},\n",
       " 28: {'word': 'high', 'token_idx': [46]},\n",
       " 29: {'word': 'serum', 'token_idx': [47]},\n",
       " 30: {'word': 'lithium', 'token_idx': [48, 49]},\n",
       " 31: {'word': 'level', 'token_idx': [50]},\n",
       " 32: {'word': 'is', 'token_idx': [51]},\n",
       " 33: {'word': 'described', 'token_idx': [52]},\n",
       " 34: {'word': '.', 'token_idx': [53]},\n",
       " 35: {'word': 'This', 'token_idx': [54]},\n",
       " 36: {'word': 'is', 'token_idx': [55]},\n",
       " 37: {'word': 'the', 'token_idx': [56]},\n",
       " 38: {'word': 'first', 'token_idx': [57]},\n",
       " 39: {'word': 'patient', 'token_idx': [58]},\n",
       " 40: {'word': 'to', 'token_idx': [59]},\n",
       " 41: {'word': 'initially', 'token_idx': [60, 61]},\n",
       " 42: {'word': 'manifest', 'token_idx': [62, 63]},\n",
       " 43: {'word': 'tricuspid', 'token_idx': [64, 65, 66]},\n",
       " 44: {'word': 'regurgitation', 'token_idx': [67, 68, 69]},\n",
       " 45: {'word': 'and', 'token_idx': [70]},\n",
       " 46: {'word': 'atrial', 'token_idx': [71]},\n",
       " 47: {'word': 'flutter', 'token_idx': [72, 73]},\n",
       " 48: {'word': ',', 'token_idx': [74]},\n",
       " 49: {'word': 'and', 'token_idx': [75]},\n",
       " 50: {'word': 'the', 'token_idx': [76]},\n",
       " 51: {'word': '11th', 'token_idx': [77, 78]},\n",
       " 52: {'word': 'described', 'token_idx': [79]},\n",
       " 53: {'word': 'patient', 'token_idx': [80]},\n",
       " 54: {'word': 'with', 'token_idx': [81]},\n",
       " 55: {'word': 'cardiac', 'token_idx': [82]},\n",
       " 56: {'word': 'disease', 'token_idx': [83]},\n",
       " 57: {'word': 'among', 'token_idx': [84]},\n",
       " 58: {'word': 'infants', 'token_idx': [85, 86]},\n",
       " 59: {'word': 'exposed', 'token_idx': [87]},\n",
       " 60: {'word': 'to', 'token_idx': [88]},\n",
       " 61: {'word': 'lithium', 'token_idx': [89, 90]},\n",
       " 62: {'word': 'compounds', 'token_idx': [91, 92]},\n",
       " 63: {'word': 'in', 'token_idx': [93]},\n",
       " 64: {'word': 'the', 'token_idx': [94]},\n",
       " 65: {'word': 'first', 'token_idx': [95]},\n",
       " 66: {'word': 'trimester', 'token_idx': [96]},\n",
       " 67: {'word': 'of', 'token_idx': [97]},\n",
       " 68: {'word': 'pregnancy', 'token_idx': [98]},\n",
       " 69: {'word': '.', 'token_idx': [99]},\n",
       " 70: {'word': 'Sixty', 'token_idx': [100]},\n",
       " 71: {'word': '-', 'token_idx': [101]},\n",
       " 72: {'word': 'three', 'token_idx': [102]},\n",
       " 73: {'word': 'percent', 'token_idx': [103]},\n",
       " 74: {'word': 'of', 'token_idx': [104]},\n",
       " 75: {'word': 'these', 'token_idx': [105]},\n",
       " 76: {'word': 'infants', 'token_idx': [106, 107]},\n",
       " 77: {'word': 'had', 'token_idx': [108]},\n",
       " 78: {'word': 'tricuspid', 'token_idx': [109, 110, 111]},\n",
       " 79: {'word': 'valve', 'token_idx': [112, 113]},\n",
       " 80: {'word': 'involvement', 'token_idx': [114, 115]},\n",
       " 81: {'word': '.', 'token_idx': [116]},\n",
       " 82: {'word': 'Lithium', 'token_idx': [117, 118]},\n",
       " 83: {'word': 'carbonate', 'token_idx': [119, 120]},\n",
       " 84: {'word': 'may', 'token_idx': [121]},\n",
       " 85: {'word': 'be', 'token_idx': [122]},\n",
       " 86: {'word': 'a', 'token_idx': [123]},\n",
       " 87: {'word': 'factor', 'token_idx': [124]},\n",
       " 88: {'word': 'in', 'token_idx': [125]},\n",
       " 89: {'word': 'the', 'token_idx': [126]},\n",
       " 90: {'word': 'increasing', 'token_idx': [127]},\n",
       " 91: {'word': 'incidence', 'token_idx': [128]},\n",
       " 92: {'word': 'of', 'token_idx': [129]},\n",
       " 93: {'word': 'congenital', 'token_idx': [130, 131]},\n",
       " 94: {'word': 'heart', 'token_idx': [132]},\n",
       " 95: {'word': 'disease', 'token_idx': [133]},\n",
       " 96: {'word': 'when', 'token_idx': [134]},\n",
       " 97: {'word': 'taken', 'token_idx': [135, 136]},\n",
       " 98: {'word': 'during', 'token_idx': [137]},\n",
       " 99: {'word': 'early', 'token_idx': [138]},\n",
       " 100: {'word': 'pregnancy', 'token_idx': [139]},\n",
       " 101: {'word': '.', 'token_idx': [140]},\n",
       " 102: {'word': 'It', 'token_idx': [141]},\n",
       " 103: {'word': 'also', 'token_idx': [142]},\n",
       " 104: {'word': 'causes', 'token_idx': [143, 144]},\n",
       " 105: {'word': 'neurologic', 'token_idx': [145, 146]},\n",
       " 106: {'word': 'depression', 'token_idx': [147]},\n",
       " 107: {'word': ',', 'token_idx': [148]},\n",
       " 108: {'word': 'cyanosis', 'token_idx': [149, 150]},\n",
       " 109: {'word': ',', 'token_idx': [151]},\n",
       " 110: {'word': 'and', 'token_idx': [152]},\n",
       " 111: {'word': 'cardiac', 'token_idx': [153]},\n",
       " 112: {'word': 'arrhythmia', 'token_idx': [154, 155]},\n",
       " 113: {'word': 'when', 'token_idx': [156]},\n",
       " 114: {'word': 'consumed', 'token_idx': [157, 158]},\n",
       " 115: {'word': 'prior', 'token_idx': [159]},\n",
       " 116: {'word': 'to', 'token_idx': [160]},\n",
       " 117: {'word': 'delivery', 'token_idx': [161]},\n",
       " 118: {'word': '.', 'token_idx': [162]}}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_token_map[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1fc29",
   "metadata": {},
   "source": [
    "## 4. Testing Out Inferencing\n",
    "- Things look fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a7298790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "test_model.to(device)\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "nb_eval_steps = 0\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "        if args.model_type != \"distilbert\":\n",
    "            inputs[\"token_type_ids\"] = (\n",
    "                batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "            )  # XLM and RoBERTa don\"t use segment_ids\n",
    "        outputs = test_model(**inputs)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            tmp_eval_loss = tmp_eval_loss.mean()\n",
    "\n",
    "    nb_eval_steps += 1\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = np.argmax(preds, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "4a42c34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 4, ..., 0, 0, 0],\n",
       "       [0, 1, 3, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 4, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the result from this live run\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7a9f2b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 512)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c618d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the prediction numpy array: (512,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 4, 4, 4, 4, 4, 4, 0, 1, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 4, 4, 4, 4, 4, 0, 2, 4, 4, 0, 2, 4, 4, 4, 4, 0,\n",
       "       0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4,\n",
       "       4, 4, 4, 4, 0, 2, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0,\n",
       "       0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4,\n",
       "       4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 4, 0, 2, 4, 0, 0, 2,\n",
       "       4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Shape of the prediction numpy array: {preds[0].shape}\")\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66060c",
   "metadata": {},
   "source": [
    "## 5. Post-processing\n",
    "- This takes in the preds (i.e. the model predictions), and combines it with the word_to_token_map to identify the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "01c6c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_and_word_location(preds, word_to_token_map):\n",
    "    '''\n",
    "    Takes in the model predictions from ConNER and the word_to_token_map to extract\n",
    "    the identified chemicals and diseases'\n",
    "     \n",
    "    Inputs:\n",
    "    - preds: model predictions\n",
    "    - word_to_token_map: a list of dictionaries showing the words and their mapping to tokenized IDs\n",
    "    \n",
    "    Output:\n",
    "    - a list of lists, showing the identfied chemical and disease entities and where to find them in the text.\n",
    "    \n",
    "    '''\n",
    "    assert preds.shape[0] == len(word_to_token_map), \"Number of samples in the predictions and mapping are different\"\n",
    "    \n",
    "    overall_entities_identified = []\n",
    "    \n",
    "    for i in range(preds.shape[0]):\n",
    "        sample_entities_identified = {}\n",
    "        pred = preds[i]\n",
    "        mapping = word_to_token_map[i]\n",
    "        \n",
    "        current_entity_type = None\n",
    "        current_entity = None\n",
    "        \n",
    "        for word_idx in mapping.keys():\n",
    "            word = mapping[word_idx]['word']\n",
    "            tokens = mapping[word_idx]['token_idx']\n",
    "            \n",
    "            \n",
    "            word_pred = pred[tokens[0]:tokens[-1]+1]\n",
    "            \n",
    "            ## cases when a starting word is found:\n",
    "            if 2 in word_pred and 1 not in word_pred:\n",
    "                assert 3 not in word_pred, f\"Error in prediction for sample {i}, the word {word} with prdictions {word_pred} predicted as B-Disease but contains I-Chemical\"\n",
    "                \n",
    "                # handles the case when a new entity immediately follows another entity\n",
    "                if current_entity_type is not None:\n",
    "                    entity_word_idx = [entity_start, entity_end]\n",
    "                    entity_word_idx_RE = [entity_start, entity_end+1]\n",
    "                    if current_entity in sample_entities_identified.keys():\n",
    "                        sample_entities_identified[current_entity]['type'].append(current_entity_type)\n",
    "                        sample_entities_identified[current_entity]['word_loc'].append(entity_word_idx)\n",
    "                        sample_entities_identified[current_entity]['RE_word_loc'].append(entity_word_idx_RE)\n",
    "                    else:\n",
    "                        sample_entities_identified[current_entity] = {'type': [current_entity_type],\n",
    "                                                                      'word_loc': [entity_word_idx],\n",
    "                                                                      'RE_word_loc': [entity_word_idx_RE]}\n",
    "                current_entity_type = 'disease'    \n",
    "                current_entity = word\n",
    "                entity_start = word_idx\n",
    "                entity_end = word_idx\n",
    "                \n",
    "            elif 1 in word_pred and 2 not in word_pred:\n",
    "                assert 4 not in word_pred, f\"Error in prediction for sample {i}, the word {word} with prdictions {word_pred} predicted as B-Chemical but contains I-Disease\"\n",
    "                \n",
    "                # handles the case when a new entity immediately follows another entity\n",
    "                if current_entity_type is not None:\n",
    "                    entity_word_idx = [entity_start, entity_end]\n",
    "                    entity_word_idx_RE = [entity_start, entity_end+1]\n",
    "                    if current_entity in sample_entities_identified.keys():\n",
    "                        sample_entities_identified[current_entity]['type'].append(current_entity_type)\n",
    "                        sample_entities_identified[current_entity]['word_loc'].append(entity_word_idx)\n",
    "                        sample_entities_identified[current_entity]['RE_word_loc'].append(entity_word_idx_RE)\n",
    "                    else:\n",
    "                        sample_entities_identified[current_entity] = {'type': [current_entity_type],\n",
    "                                                                      'word_loc': [entity_word_idx],\n",
    "                                                                      'RE_word_loc': [entity_word_idx_RE]}\n",
    "                current_entity_type = 'chemical'    \n",
    "                current_entity = word\n",
    "                entity_start = word_idx\n",
    "                entity_end = word_idx\n",
    "            \n",
    "            ## cases when a middle word is found:\n",
    "            elif 2 not in word_pred and 4 in word_pred:\n",
    "                assert 3 not in word_pred, f\"Error in prediction for sample {i}, the word {word} with prdictions {word_pred} predicted as having multiple classes\"\n",
    "                assert current_entity_type == 'disease' , f\"Error in prediction for sample {i}, the word {word} with prdictions {word_pred} predicted as disease but follows a previous word of {current_entity_type} class\"\n",
    "                current_entity = current_entity + \" \" + word\n",
    "                entity_end = word_idx\n",
    "            \n",
    "            elif 1 not in word_pred and 3 in word_pred:\n",
    "                assert 4 not in word_pred, f\"Error in prediction for sample {i}, the word {word} with prdictions {word_pred} predicted as having multiple classes\"\n",
    "                assert current_entity_type == 'chemical' , f\"Error in prediction for sample {i}, the word {word} with prdictions {word_pred} predicted as chemical but follows a previous word of {current_entity_type} class\"\n",
    "                current_entity = current_entity + \" \" + word\n",
    "                entity_end = word_idx\n",
    "                \n",
    "            ## cases when a middle word is found:\n",
    "            elif np.mean(word_pred) == 0:\n",
    "                if current_entity_type is not None:\n",
    "                    entity_word_idx = [entity_start, entity_end]\n",
    "                    entity_word_idx_RE = [entity_start, entity_end+1]\n",
    "                    if current_entity in sample_entities_identified.keys():\n",
    "                        sample_entities_identified[current_entity]['type'].append(current_entity_type)\n",
    "                        sample_entities_identified[current_entity]['word_loc'].append(entity_word_idx)\n",
    "                        sample_entities_identified[current_entity]['RE_word_loc'].append(entity_word_idx_RE)\n",
    "                    else:\n",
    "                        sample_entities_identified[current_entity] = {'type': [current_entity_type],\n",
    "                                                                      'word_loc': [entity_word_idx],\n",
    "                                                                      'RE_word_loc': [entity_word_idx_RE]}\n",
    "                        \n",
    "                current_entity_type = None\n",
    "                current_entity = None\n",
    "\n",
    "            else:\n",
    "                print(f\"Unexpected prediction case for word {word}, at tokens {tokens}, from sample {i}\")\n",
    "                \n",
    "        overall_entities_identified.append(sample_entities_identified)   \n",
    "        \n",
    "    return overall_entities_identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "0a4c3b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected prediction case for word range, at tokens [512], from sample 22\n",
      "Unexpected prediction case for word 84, at tokens [513], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [514], from sample 22\n",
      "Unexpected prediction case for word 98, at tokens [515], from sample 22\n",
      "Unexpected prediction case for word ), at tokens [516], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [517], from sample 22\n",
      "Unexpected prediction case for word The, at tokens [518], from sample 22\n",
      "Unexpected prediction case for word mean, at tokens [519], from sample 22\n",
      "Unexpected prediction case for word BIS, at tokens [520], from sample 22\n",
      "Unexpected prediction case for word score, at tokens [521], from sample 22\n",
      "Unexpected prediction case for word corresponding, at tokens [522], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [523], from sample 22\n",
      "Unexpected prediction case for word the, at tokens [524], from sample 22\n",
      "Unexpected prediction case for word first, at tokens [525], from sample 22\n",
      "Unexpected prediction case for word word, at tokens [526], from sample 22\n",
      "Unexpected prediction case for word recalled, at tokens [527, 528], from sample 22\n",
      "Unexpected prediction case for word after, at tokens [529], from sample 22\n",
      "Unexpected prediction case for word the, at tokens [530], from sample 22\n",
      "Unexpected prediction case for word procedure, at tokens [531], from sample 22\n",
      "Unexpected prediction case for word was, at tokens [532], from sample 22\n",
      "Unexpected prediction case for word completed, at tokens [533, 534], from sample 22\n",
      "Unexpected prediction case for word was, at tokens [535], from sample 22\n",
      "Unexpected prediction case for word 91, at tokens [536], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [537], from sample 22\n",
      "Unexpected prediction case for word 2, at tokens [538], from sample 22\n",
      "Unexpected prediction case for word (, at tokens [539], from sample 22\n",
      "Unexpected prediction case for word 95, at tokens [540], from sample 22\n",
      "Unexpected prediction case for word %, at tokens [541], from sample 22\n",
      "Unexpected prediction case for word CI, at tokens [542], from sample 22\n",
      "Unexpected prediction case for word =, at tokens [543], from sample 22\n",
      "Unexpected prediction case for word 88, at tokens [544], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [545], from sample 22\n",
      "Unexpected prediction case for word 1, at tokens [546], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [547], from sample 22\n",
      "Unexpected prediction case for word 94, at tokens [548], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [549], from sample 22\n",
      "Unexpected prediction case for word 3, at tokens [550], from sample 22\n",
      "Unexpected prediction case for word ), at tokens [551], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [552], from sample 22\n",
      "Unexpected prediction case for word All, at tokens [553], from sample 22\n",
      "Unexpected prediction case for word patients, at tokens [554], from sample 22\n",
      "Unexpected prediction case for word recalled, at tokens [555, 556], from sample 22\n",
      "Unexpected prediction case for word at, at tokens [557], from sample 22\n",
      "Unexpected prediction case for word least, at tokens [558, 559], from sample 22\n",
      "Unexpected prediction case for word one, at tokens [560], from sample 22\n",
      "Unexpected prediction case for word word, at tokens [561], from sample 22\n",
      "Unexpected prediction case for word that, at tokens [562], from sample 22\n",
      "Unexpected prediction case for word had, at tokens [563], from sample 22\n",
      "Unexpected prediction case for word been, at tokens [564, 565], from sample 22\n",
      "Unexpected prediction case for word read, at tokens [566], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [567], from sample 22\n",
      "Unexpected prediction case for word them, at tokens [568, 569], from sample 22\n",
      "Unexpected prediction case for word during, at tokens [570], from sample 22\n",
      "Unexpected prediction case for word the, at tokens [571], from sample 22\n",
      "Unexpected prediction case for word protocol, at tokens [572], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [573], from sample 22\n",
      "Unexpected prediction case for word The, at tokens [574], from sample 22\n",
      "Unexpected prediction case for word mean, at tokens [575], from sample 22\n",
      "Unexpected prediction case for word lowest, at tokens [576, 577], from sample 22\n",
      "Unexpected prediction case for word BIS, at tokens [578], from sample 22\n",
      "Unexpected prediction case for word score, at tokens [579], from sample 22\n",
      "Unexpected prediction case for word for, at tokens [580], from sample 22\n",
      "Unexpected prediction case for word any, at tokens [581], from sample 22\n",
      "Unexpected prediction case for word recalled, at tokens [582, 583], from sample 22\n",
      "Unexpected prediction case for word word, at tokens [584], from sample 22\n",
      "Unexpected prediction case for word was, at tokens [585], from sample 22\n",
      "Unexpected prediction case for word 91, at tokens [586], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [587], from sample 22\n",
      "Unexpected prediction case for word 5, at tokens [588], from sample 22\n",
      "Unexpected prediction case for word (, at tokens [589], from sample 22\n",
      "Unexpected prediction case for word +, at tokens [590], from sample 22\n",
      "Unexpected prediction case for word /, at tokens [591], from sample 22\n",
      "Unexpected prediction case for word -, at tokens [592], from sample 22\n",
      "Unexpected prediction case for word 11, at tokens [593], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [594], from sample 22\n",
      "Unexpected prediction case for word 1, at tokens [595], from sample 22\n",
      "Unexpected prediction case for word ;, at tokens [596], from sample 22\n",
      "Unexpected prediction case for word range, at tokens [597], from sample 22\n",
      "Unexpected prediction case for word 79, at tokens [598], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [599], from sample 22\n",
      "Unexpected prediction case for word 98, at tokens [600], from sample 22\n",
      "Unexpected prediction case for word ), at tokens [601], from sample 22\n",
      "Unexpected prediction case for word ,, at tokens [602], from sample 22\n",
      "Unexpected prediction case for word and, at tokens [603], from sample 22\n",
      "Unexpected prediction case for word no, at tokens [604], from sample 22\n",
      "Unexpected prediction case for word words, at tokens [605], from sample 22\n",
      "Unexpected prediction case for word were, at tokens [606], from sample 22\n",
      "Unexpected prediction case for word recalled, at tokens [607, 608], from sample 22\n",
      "Unexpected prediction case for word when, at tokens [609], from sample 22\n",
      "Unexpected prediction case for word the, at tokens [610], from sample 22\n",
      "Unexpected prediction case for word corresponding, at tokens [611], from sample 22\n",
      "Unexpected prediction case for word BIS, at tokens [612], from sample 22\n",
      "Unexpected prediction case for word score, at tokens [613], from sample 22\n",
      "Unexpected prediction case for word was, at tokens [614], from sample 22\n",
      "Unexpected prediction case for word less, at tokens [615], from sample 22\n",
      "Unexpected prediction case for word than, at tokens [616], from sample 22\n",
      "Unexpected prediction case for word 90, at tokens [617], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [618], from sample 22\n",
      "Unexpected prediction case for word CONCLUSIONS, at tokens [619], from sample 22\n",
      "Unexpected prediction case for word :, at tokens [620], from sample 22\n",
      "Unexpected prediction case for word There, at tokens [621], from sample 22\n",
      "Unexpected prediction case for word is, at tokens [622], from sample 22\n",
      "Unexpected prediction case for word a, at tokens [623], from sample 22\n",
      "Unexpected prediction case for word range, at tokens [624], from sample 22\n",
      "Unexpected prediction case for word of, at tokens [625], from sample 22\n",
      "Unexpected prediction case for word BIS, at tokens [626], from sample 22\n",
      "Unexpected prediction case for word scores, at tokens [627], from sample 22\n",
      "Unexpected prediction case for word during, at tokens [628], from sample 22\n",
      "Unexpected prediction case for word which, at tokens [629], from sample 22\n",
      "Unexpected prediction case for word sedated, at tokens [630, 631], from sample 22\n",
      "Unexpected prediction case for word patients, at tokens [632], from sample 22\n",
      "Unexpected prediction case for word are, at tokens [633], from sample 22\n",
      "Unexpected prediction case for word able, at tokens [634], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [635], from sample 22\n",
      "Unexpected prediction case for word repeat, at tokens [636], from sample 22\n",
      "Unexpected prediction case for word words, at tokens [637], from sample 22\n",
      "Unexpected prediction case for word read, at tokens [638], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [639], from sample 22\n",
      "Unexpected prediction case for word them, at tokens [640, 641], from sample 22\n",
      "Unexpected prediction case for word but, at tokens [642], from sample 22\n",
      "Unexpected prediction case for word are, at tokens [643], from sample 22\n",
      "Unexpected prediction case for word not, at tokens [644], from sample 22\n",
      "Unexpected prediction case for word able, at tokens [645], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [646], from sample 22\n",
      "Unexpected prediction case for word subsequently, at tokens [647, 648], from sample 22\n",
      "Unexpected prediction case for word recall, at tokens [649, 650], from sample 22\n",
      "Unexpected prediction case for word these, at tokens [651], from sample 22\n",
      "Unexpected prediction case for word words, at tokens [652], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [653], from sample 22\n",
      "Unexpected prediction case for word Furthermore, at tokens [654], from sample 22\n",
      "Unexpected prediction case for word ,, at tokens [655], from sample 22\n",
      "Unexpected prediction case for word patients, at tokens [656], from sample 22\n",
      "Unexpected prediction case for word had, at tokens [657], from sample 22\n",
      "Unexpected prediction case for word no, at tokens [658], from sample 22\n",
      "Unexpected prediction case for word recall, at tokens [659, 660], from sample 22\n",
      "Unexpected prediction case for word of, at tokens [661], from sample 22\n",
      "Unexpected prediction case for word words, at tokens [662], from sample 22\n",
      "Unexpected prediction case for word repeated, at tokens [663, 664], from sample 22\n",
      "Unexpected prediction case for word prior, at tokens [665], from sample 22\n",
      "Unexpected prediction case for word to, at tokens [666], from sample 22\n",
      "Unexpected prediction case for word procedural, at tokens [667], from sample 22\n",
      "Unexpected prediction case for word sedation, at tokens [668, 669], from sample 22\n",
      "Unexpected prediction case for word in, at tokens [670], from sample 22\n",
      "Unexpected prediction case for word BIS, at tokens [671], from sample 22\n",
      "Unexpected prediction case for word ranges, at tokens [672, 673], from sample 22\n",
      "Unexpected prediction case for word associated, at tokens [674], from sample 22\n",
      "Unexpected prediction case for word with, at tokens [675], from sample 22\n",
      "Unexpected prediction case for word recall, at tokens [676, 677], from sample 22\n",
      "Unexpected prediction case for word after, at tokens [678], from sample 22\n",
      "Unexpected prediction case for word procedural, at tokens [679], from sample 22\n",
      "Unexpected prediction case for word sedation, at tokens [680, 681], from sample 22\n",
      "Unexpected prediction case for word ,, at tokens [682], from sample 22\n",
      "Unexpected prediction case for word suggestive, at tokens [683, 684, 685], from sample 22\n",
      "Unexpected prediction case for word of, at tokens [686], from sample 22\n",
      "Unexpected prediction case for word retrograde, at tokens [687, 688], from sample 22\n",
      "Unexpected prediction case for word amnesia, at tokens [689, 690, 691], from sample 22\n",
      "Unexpected prediction case for word ., at tokens [692], from sample 22\n",
      "Unexpected prediction case for word one, at tokens [512], from sample 27\n",
      "Unexpected prediction case for word presented, at tokens [513, 514], from sample 27\n",
      "Unexpected prediction case for word with, at tokens [515], from sample 27\n",
      "Unexpected prediction case for word an, at tokens [516], from sample 27\n",
      "Unexpected prediction case for word auditory, at tokens [517, 518], from sample 27\n",
      "Unexpected prediction case for word loss, at tokens [519], from sample 27\n",
      "Unexpected prediction case for word of, at tokens [520], from sample 27\n",
      "Unexpected prediction case for word -, at tokens [521], from sample 27\n",
      "Unexpected prediction case for word 30, at tokens [522], from sample 27\n",
      "Unexpected prediction case for word dB, at tokens [523], from sample 27\n",
      "Unexpected prediction case for word ,, at tokens [524], from sample 27\n",
      "Unexpected prediction case for word whereas, at tokens [525, 526], from sample 27\n",
      "Unexpected prediction case for word in, at tokens [527], from sample 27\n",
      "Unexpected prediction case for word the, at tokens [528], from sample 27\n",
      "Unexpected prediction case for word OD, at tokens [529], from sample 27\n",
      "Unexpected prediction case for word group, at tokens [530], from sample 27\n",
      "Unexpected prediction case for word only, at tokens [531], from sample 27\n",
      "Unexpected prediction case for word one, at tokens [532], from sample 27\n",
      "Unexpected prediction case for word patient, at tokens [533], from sample 27\n",
      "Unexpected prediction case for word presented, at tokens [534, 535], from sample 27\n",
      "Unexpected prediction case for word decreased, at tokens [536], from sample 27\n",
      "Unexpected prediction case for word auditory, at tokens [537, 538], from sample 27\n",
      "Unexpected prediction case for word function, at tokens [539], from sample 27\n",
      "Unexpected prediction case for word ., at tokens [540], from sample 27\n",
      "Unexpected prediction case for word CONCLUSION, at tokens [541], from sample 27\n",
      "Unexpected prediction case for word :, at tokens [542], from sample 27\n",
      "Unexpected prediction case for word This, at tokens [543], from sample 27\n",
      "Unexpected prediction case for word small, at tokens [544], from sample 27\n",
      "Unexpected prediction case for word study, at tokens [545], from sample 27\n",
      "Unexpected prediction case for word suggests, at tokens [546, 547, 548], from sample 27\n",
      "Unexpected prediction case for word that, at tokens [549], from sample 27\n",
      "Unexpected prediction case for word a, at tokens [550], from sample 27\n",
      "Unexpected prediction case for word once, at tokens [551, 552], from sample 27\n",
      "Unexpected prediction case for word -, at tokens [553], from sample 27\n",
      "Unexpected prediction case for word daily, at tokens [554], from sample 27\n",
      "Unexpected prediction case for word dosing, at tokens [555, 556], from sample 27\n",
      "Unexpected prediction case for word regimen, at tokens [557, 558], from sample 27\n",
      "Unexpected prediction case for word of, at tokens [559], from sample 27\n",
      "Unexpected prediction case for word tobramycin, at tokens [560, 561, 562], from sample 27\n",
      "Unexpected prediction case for word is, at tokens [563], from sample 27\n",
      "Unexpected prediction case for word at, at tokens [564], from sample 27\n",
      "Unexpected prediction case for word least, at tokens [565, 566], from sample 27\n",
      "Unexpected prediction case for word as, at tokens [567], from sample 27\n",
      "Unexpected prediction case for word effective, at tokens [568], from sample 27\n",
      "Unexpected prediction case for word as, at tokens [569], from sample 27\n",
      "Unexpected prediction case for word and, at tokens [570], from sample 27\n",
      "Unexpected prediction case for word is, at tokens [571], from sample 27\n",
      "Unexpected prediction case for word no, at tokens [572], from sample 27\n",
      "Unexpected prediction case for word more, at tokens [573], from sample 27\n",
      "Unexpected prediction case for word and, at tokens [574], from sample 27\n",
      "Unexpected prediction case for word possibly, at tokens [575], from sample 27\n",
      "Unexpected prediction case for word less, at tokens [576], from sample 27\n",
      "Unexpected prediction case for word toxic, at tokens [577], from sample 27\n",
      "Unexpected prediction case for word than, at tokens [578], from sample 27\n",
      "Unexpected prediction case for word the, at tokens [579], from sample 27\n",
      "Unexpected prediction case for word twice, at tokens [580, 581], from sample 27\n",
      "Unexpected prediction case for word -, at tokens [582], from sample 27\n",
      "Unexpected prediction case for word daily, at tokens [583], from sample 27\n",
      "Unexpected prediction case for word regimen, at tokens [584, 585], from sample 27\n",
      "Unexpected prediction case for word ., at tokens [586], from sample 27\n",
      "Unexpected prediction case for word Using, at tokens [587], from sample 27\n",
      "Unexpected prediction case for word a, at tokens [588], from sample 27\n",
      "Unexpected prediction case for word single, at tokens [589], from sample 27\n",
      "Unexpected prediction case for word -, at tokens [590], from sample 27\n",
      "Unexpected prediction case for word dose, at tokens [591], from sample 27\n",
      "Unexpected prediction case for word therapy, at tokens [592], from sample 27\n",
      "Unexpected prediction case for word ,, at tokens [593], from sample 27\n",
      "Unexpected prediction case for word peak, at tokens [594], from sample 27\n",
      "Unexpected prediction case for word concentration, at tokens [595], from sample 27\n",
      "Unexpected prediction case for word determination, at tokens [596], from sample 27\n",
      "Unexpected prediction case for word is, at tokens [597], from sample 27\n",
      "Unexpected prediction case for word not, at tokens [598], from sample 27\n",
      "Unexpected prediction case for word necessary, at tokens [599, 600], from sample 27\n",
      "Unexpected prediction case for word ,, at tokens [601], from sample 27\n",
      "Unexpected prediction case for word only, at tokens [602], from sample 27\n",
      "Unexpected prediction case for word trough, at tokens [603, 604], from sample 27\n",
      "Unexpected prediction case for word samples, at tokens [605], from sample 27\n",
      "Unexpected prediction case for word should, at tokens [606, 607], from sample 27\n",
      "Unexpected prediction case for word be, at tokens [608], from sample 27\n",
      "Unexpected prediction case for word monitored, at tokens [609, 610, 611], from sample 27\n",
      "Unexpected prediction case for word to, at tokens [612], from sample 27\n",
      "Unexpected prediction case for word ensure, at tokens [613, 614], from sample 27\n",
      "Unexpected prediction case for word levels, at tokens [615], from sample 27\n",
      "Unexpected prediction case for word below, at tokens [616], from sample 27\n",
      "Unexpected prediction case for word 2, at tokens [617], from sample 27\n",
      "Unexpected prediction case for word microg, at tokens [618, 619], from sample 27\n",
      "Unexpected prediction case for word /, at tokens [620], from sample 27\n",
      "Unexpected prediction case for word ml, at tokens [621], from sample 27\n",
      "Unexpected prediction case for word ., at tokens [622], from sample 27\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Error in prediction for sample 28, the word decrease with prdictions [0 4] predicted as disease but follows a previous word of None class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [281]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testing \u001b[38;5;241m=\u001b[39m \u001b[43mextract_entity_and_word_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_to_token_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [280]\u001b[0m, in \u001b[0;36mextract_entity_and_word_location\u001b[1;34m(preds, word_to_token_map)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word_pred \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01min\u001b[39;00m word_pred:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word_pred, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in prediction for sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, the word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with prdictions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_pred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m predicted as having multiple classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m current_entity_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisease\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in prediction for sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, the word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with prdictions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_pred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m predicted as disease but follows a previous word of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_entity_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m     current_entity \u001b[38;5;241m=\u001b[39m current_entity \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m word\n\u001b[0;32m     79\u001b[0m     entity_end \u001b[38;5;241m=\u001b[39m word_idx\n",
      "\u001b[1;31mAssertionError\u001b[0m: Error in prediction for sample 28, the word decrease with prdictions [0 4] predicted as disease but follows a previous word of None class"
     ]
    }
   ],
   "source": [
    "testing = extract_entity_and_word_location(preds, word_to_token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "eccb8b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0,\n",
       "       1, 3, 3, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 4, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3,\n",
       "       3, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 1, 3,\n",
       "       3, 3, 0, 0, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 1, 3, 3, 0, 0, 2, 4, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "36dadba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'word': 'Chronic', 'token_idx': [1]},\n",
       " 1: {'word': 'effects', 'token_idx': [2]},\n",
       " 2: {'word': 'of', 'token_idx': [3]},\n",
       " 3: {'word': 'a', 'token_idx': [4]},\n",
       " 4: {'word': 'novel', 'token_idx': [5]},\n",
       " 5: {'word': 'synthetic', 'token_idx': [6, 7]},\n",
       " 6: {'word': 'anthracycline', 'token_idx': [8, 9]},\n",
       " 7: {'word': 'derivative', 'token_idx': [10, 11]},\n",
       " 8: {'word': '(', 'token_idx': [12]},\n",
       " 9: {'word': 'SM', 'token_idx': [13]},\n",
       " 10: {'word': '-', 'token_idx': [14]},\n",
       " 11: {'word': '5887', 'token_idx': [15, 16]},\n",
       " 12: {'word': ')', 'token_idx': [17]},\n",
       " 13: {'word': 'on', 'token_idx': [18]},\n",
       " 14: {'word': 'normal', 'token_idx': [19]},\n",
       " 15: {'word': 'heart', 'token_idx': [20]},\n",
       " 16: {'word': 'and', 'token_idx': [21]},\n",
       " 17: {'word': 'doxorubicin', 'token_idx': [22, 23, 24]},\n",
       " 18: {'word': '-', 'token_idx': [25]},\n",
       " 19: {'word': 'induced', 'token_idx': [26]},\n",
       " 20: {'word': 'cardiomyopathy', 'token_idx': [27, 28, 29]},\n",
       " 21: {'word': 'in', 'token_idx': [30]},\n",
       " 22: {'word': 'beagle', 'token_idx': [31, 32]},\n",
       " 23: {'word': 'dogs', 'token_idx': [33, 34]},\n",
       " 24: {'word': '.', 'token_idx': [35]},\n",
       " 25: {'word': 'This', 'token_idx': [36]},\n",
       " 26: {'word': 'study', 'token_idx': [37]},\n",
       " 27: {'word': 'was', 'token_idx': [38]},\n",
       " 28: {'word': 'designed', 'token_idx': [39]},\n",
       " 29: {'word': 'to', 'token_idx': [40]},\n",
       " 30: {'word': 'investigate', 'token_idx': [41, 42, 43]},\n",
       " 31: {'word': 'the', 'token_idx': [44]},\n",
       " 32: {'word': 'chronic', 'token_idx': [45]},\n",
       " 33: {'word': 'cardiotoxic', 'token_idx': [46, 47]},\n",
       " 34: {'word': 'potential', 'token_idx': [48]},\n",
       " 35: {'word': 'of', 'token_idx': [49]},\n",
       " 36: {'word': 'SM', 'token_idx': [50]},\n",
       " 37: {'word': '-', 'token_idx': [51]},\n",
       " 38: {'word': '5887', 'token_idx': [52, 53]},\n",
       " 39: {'word': 'and', 'token_idx': [54]},\n",
       " 40: {'word': 'a', 'token_idx': [55]},\n",
       " 41: {'word': 'possible', 'token_idx': [56, 57]},\n",
       " 42: {'word': 'deteriorating', 'token_idx': [58, 59, 60]},\n",
       " 43: {'word': 'effect', 'token_idx': [61]},\n",
       " 44: {'word': 'of', 'token_idx': [62]},\n",
       " 45: {'word': 'SM', 'token_idx': [63]},\n",
       " 46: {'word': '-', 'token_idx': [64]},\n",
       " 47: {'word': '5887', 'token_idx': [65, 66]},\n",
       " 48: {'word': 'on', 'token_idx': [67]},\n",
       " 49: {'word': 'low', 'token_idx': [68]},\n",
       " 50: {'word': '-', 'token_idx': [69]},\n",
       " 51: {'word': 'grade', 'token_idx': [70]},\n",
       " 52: {'word': 'cardiotoxicity', 'token_idx': [71, 72]},\n",
       " 53: {'word': 'pre', 'token_idx': [73]},\n",
       " 54: {'word': '-', 'token_idx': [74]},\n",
       " 55: {'word': 'induced', 'token_idx': [75]},\n",
       " 56: {'word': 'by', 'token_idx': [76]},\n",
       " 57: {'word': 'doxorubicin', 'token_idx': [77, 78, 79]},\n",
       " 58: {'word': 'in', 'token_idx': [80]},\n",
       " 59: {'word': 'beagle', 'token_idx': [81, 82]},\n",
       " 60: {'word': 'dogs', 'token_idx': [83, 84]},\n",
       " 61: {'word': '.', 'token_idx': [85]},\n",
       " 62: {'word': 'In', 'token_idx': [86]},\n",
       " 63: {'word': 'the', 'token_idx': [87]},\n",
       " 64: {'word': 'chronic', 'token_idx': [88]},\n",
       " 65: {'word': 'treatment', 'token_idx': [89]},\n",
       " 66: {'word': ',', 'token_idx': [90]},\n",
       " 67: {'word': 'beagle', 'token_idx': [91, 92]},\n",
       " 68: {'word': 'dogs', 'token_idx': [93, 94]},\n",
       " 69: {'word': 'of', 'token_idx': [95]},\n",
       " 70: {'word': 'each', 'token_idx': [96]},\n",
       " 71: {'word': 'sex', 'token_idx': [97]},\n",
       " 72: {'word': 'were', 'token_idx': [98]},\n",
       " 73: {'word': 'given', 'token_idx': [99]},\n",
       " 74: {'word': 'intravenously', 'token_idx': [100, 101, 102]},\n",
       " 75: {'word': 'once', 'token_idx': [103, 104]},\n",
       " 76: {'word': 'every', 'token_idx': [105]},\n",
       " 77: {'word': '3', 'token_idx': [106]},\n",
       " 78: {'word': 'weeks', 'token_idx': [107]},\n",
       " 79: {'word': ',', 'token_idx': [108]},\n",
       " 80: {'word': 'either', 'token_idx': [109]},\n",
       " 81: {'word': 'a', 'token_idx': [110]},\n",
       " 82: {'word': 'sublethal', 'token_idx': [111, 112, 113]},\n",
       " 83: {'word': 'dose', 'token_idx': [114]},\n",
       " 84: {'word': 'of', 'token_idx': [115]},\n",
       " 85: {'word': 'doxorubicin', 'token_idx': [116, 117, 118]},\n",
       " 86: {'word': '(', 'token_idx': [119]},\n",
       " 87: {'word': '1', 'token_idx': [120]},\n",
       " 88: {'word': '.', 'token_idx': [121]},\n",
       " 89: {'word': '5', 'token_idx': [122]},\n",
       " 90: {'word': 'mg', 'token_idx': [123]},\n",
       " 91: {'word': '/', 'token_idx': [124]},\n",
       " 92: {'word': 'kg', 'token_idx': [125]},\n",
       " 93: {'word': ')', 'token_idx': [126]},\n",
       " 94: {'word': 'or', 'token_idx': [127]},\n",
       " 95: {'word': 'SM', 'token_idx': [128]},\n",
       " 96: {'word': '-', 'token_idx': [129]},\n",
       " 97: {'word': '5887', 'token_idx': [130, 131]},\n",
       " 98: {'word': '(', 'token_idx': [132]},\n",
       " 99: {'word': '2', 'token_idx': [133]},\n",
       " 100: {'word': '.', 'token_idx': [134]},\n",
       " 101: {'word': '5', 'token_idx': [135]},\n",
       " 102: {'word': 'mg', 'token_idx': [136]},\n",
       " 103: {'word': '/', 'token_idx': [137]},\n",
       " 104: {'word': 'kg', 'token_idx': [138]},\n",
       " 105: {'word': ')', 'token_idx': [139]},\n",
       " 106: {'word': '.', 'token_idx': [140]},\n",
       " 107: {'word': 'The', 'token_idx': [141]},\n",
       " 108: {'word': 'experiment', 'token_idx': [142]},\n",
       " 109: {'word': 'was', 'token_idx': [143]},\n",
       " 110: {'word': 'terminated', 'token_idx': [144]},\n",
       " 111: {'word': '3', 'token_idx': [145]},\n",
       " 112: {'word': 'weeks', 'token_idx': [146]},\n",
       " 113: {'word': 'after', 'token_idx': [147]},\n",
       " 114: {'word': 'the', 'token_idx': [148]},\n",
       " 115: {'word': 'ninth', 'token_idx': [149, 150]},\n",
       " 116: {'word': 'dosing', 'token_idx': [151, 152]},\n",
       " 117: {'word': '.', 'token_idx': [153]},\n",
       " 118: {'word': 'Animals', 'token_idx': [154]},\n",
       " 119: {'word': 'which', 'token_idx': [155]},\n",
       " 120: {'word': 'received', 'token_idx': [156, 157]},\n",
       " 121: {'word': 'over', 'token_idx': [158]},\n",
       " 122: {'word': 'six', 'token_idx': [159]},\n",
       " 123: {'word': 'courses', 'token_idx': [160, 161]},\n",
       " 124: {'word': 'of', 'token_idx': [162]},\n",
       " 125: {'word': 'doxorubicin', 'token_idx': [163, 164, 165]},\n",
       " 126: {'word': 'demonstrated', 'token_idx': [166, 167, 168]},\n",
       " 127: {'word': 'the', 'token_idx': [169]},\n",
       " 128: {'word': 'electrocardiogram', 'token_idx': [170, 171, 172]},\n",
       " 129: {'word': '(', 'token_idx': [173]},\n",
       " 130: {'word': 'ECG', 'token_idx': [174]},\n",
       " 131: {'word': ')', 'token_idx': [175]},\n",
       " 132: {'word': 'changes', 'token_idx': [176]},\n",
       " 133: {'word': ',', 'token_idx': [177]},\n",
       " 134: {'word': 'decrease', 'token_idx': [178, 179]},\n",
       " 135: {'word': 'of', 'token_idx': [180]},\n",
       " 136: {'word': 'blood', 'token_idx': [181]},\n",
       " 137: {'word': 'pressure', 'token_idx': [182]},\n",
       " 138: {'word': 'and', 'token_idx': [183]},\n",
       " 139: {'word': 'high', 'token_idx': [184]},\n",
       " 140: {'word': '-', 'token_idx': [185]},\n",
       " 141: {'word': 'grade', 'token_idx': [186]},\n",
       " 142: {'word': 'histopathological', 'token_idx': [187, 188]},\n",
       " 143: {'word': 'cardiomyopathy', 'token_idx': [189, 190, 191]},\n",
       " 144: {'word': ',', 'token_idx': [192]},\n",
       " 145: {'word': 'while', 'token_idx': [193]},\n",
       " 146: {'word': 'animals', 'token_idx': [194, 195]},\n",
       " 147: {'word': 'which', 'token_idx': [196]},\n",
       " 148: {'word': 'were', 'token_idx': [197]},\n",
       " 149: {'word': 'terminally', 'token_idx': [198]},\n",
       " 150: {'word': 'sacrificed', 'token_idx': [199, 200, 201, 202]},\n",
       " 151: {'word': 'after', 'token_idx': [203]},\n",
       " 152: {'word': 'the', 'token_idx': [204]},\n",
       " 153: {'word': 'SM', 'token_idx': [205]},\n",
       " 154: {'word': '-', 'token_idx': [206]},\n",
       " 155: {'word': '5887', 'token_idx': [207, 208]},\n",
       " 156: {'word': 'administration', 'token_idx': [209]},\n",
       " 157: {'word': 'did', 'token_idx': [210]},\n",
       " 158: {'word': 'not', 'token_idx': [211]},\n",
       " 159: {'word': 'show', 'token_idx': [212, 213]},\n",
       " 160: {'word': 'any', 'token_idx': [214]},\n",
       " 161: {'word': 'changes', 'token_idx': [215]},\n",
       " 162: {'word': 'in', 'token_idx': [216]},\n",
       " 163: {'word': 'ECG', 'token_idx': [217]},\n",
       " 164: {'word': ',', 'token_idx': [218]},\n",
       " 165: {'word': 'blood', 'token_idx': [219]},\n",
       " 166: {'word': 'pressure', 'token_idx': [220]},\n",
       " 167: {'word': 'and', 'token_idx': [221]},\n",
       " 168: {'word': 'histopathological', 'token_idx': [222, 223]},\n",
       " 169: {'word': 'examinations', 'token_idx': [224, 225, 226]},\n",
       " 170: {'word': '.', 'token_idx': [227]},\n",
       " 171: {'word': 'To', 'token_idx': [228]},\n",
       " 172: {'word': 'examine', 'token_idx': [229, 230]},\n",
       " 173: {'word': 'a', 'token_idx': [231]},\n",
       " 174: {'word': 'possibly', 'token_idx': [232]},\n",
       " 175: {'word': 'deteriorating', 'token_idx': [233, 234, 235]},\n",
       " 176: {'word': 'cardiotoxic', 'token_idx': [236, 237]},\n",
       " 177: {'word': 'effect', 'token_idx': [238]},\n",
       " 178: {'word': 'of', 'token_idx': [239]},\n",
       " 179: {'word': 'SM', 'token_idx': [240]},\n",
       " 180: {'word': '-', 'token_idx': [241]},\n",
       " 181: {'word': '5887', 'token_idx': [242, 243]},\n",
       " 182: {'word': ',', 'token_idx': [244]},\n",
       " 183: {'word': 'low', 'token_idx': [245]},\n",
       " 184: {'word': '-', 'token_idx': [246]},\n",
       " 185: {'word': 'grade', 'token_idx': [247]},\n",
       " 186: {'word': 'cardiomyopathy', 'token_idx': [248, 249, 250]},\n",
       " 187: {'word': 'was', 'token_idx': [251]},\n",
       " 188: {'word': 'induced', 'token_idx': [252]},\n",
       " 189: {'word': 'in', 'token_idx': [253]},\n",
       " 190: {'word': 'dogs', 'token_idx': [254, 255]},\n",
       " 191: {'word': 'by', 'token_idx': [256]},\n",
       " 192: {'word': 'four', 'token_idx': [257]},\n",
       " 193: {'word': 'courses', 'token_idx': [258, 259]},\n",
       " 194: {'word': 'of', 'token_idx': [260]},\n",
       " 195: {'word': 'doxorubicin', 'token_idx': [261, 262, 263]},\n",
       " 196: {'word': '(', 'token_idx': [264]},\n",
       " 197: {'word': '1', 'token_idx': [265]},\n",
       " 198: {'word': '.', 'token_idx': [266]},\n",
       " 199: {'word': '5', 'token_idx': [267]},\n",
       " 200: {'word': 'mg', 'token_idx': [268]},\n",
       " 201: {'word': '/', 'token_idx': [269]},\n",
       " 202: {'word': 'kg', 'token_idx': [270]},\n",
       " 203: {'word': ')', 'token_idx': [271]},\n",
       " 204: {'word': '.', 'token_idx': [272]},\n",
       " 205: {'word': 'Nine', 'token_idx': [273]},\n",
       " 206: {'word': 'weeks', 'token_idx': [274]},\n",
       " 207: {'word': 'after', 'token_idx': [275]},\n",
       " 208: {'word': 'pre', 'token_idx': [276]},\n",
       " 209: {'word': '-', 'token_idx': [277]},\n",
       " 210: {'word': 'treatment', 'token_idx': [278]},\n",
       " 211: {'word': ',', 'token_idx': [279]},\n",
       " 212: {'word': 'dogs', 'token_idx': [280, 281]},\n",
       " 213: {'word': 'were', 'token_idx': [282]},\n",
       " 214: {'word': 'given', 'token_idx': [283]},\n",
       " 215: {'word': 'four', 'token_idx': [284]},\n",
       " 216: {'word': 'courses', 'token_idx': [285, 286]},\n",
       " 217: {'word': 'of', 'token_idx': [287]},\n",
       " 218: {'word': 'either', 'token_idx': [288]},\n",
       " 219: {'word': 'doxorubicin', 'token_idx': [289, 290, 291]},\n",
       " 220: {'word': '(', 'token_idx': [292]},\n",
       " 221: {'word': '1', 'token_idx': [293]},\n",
       " 222: {'word': '.', 'token_idx': [294]},\n",
       " 223: {'word': '5', 'token_idx': [295]},\n",
       " 224: {'word': 'mg', 'token_idx': [296]},\n",
       " 225: {'word': '/', 'token_idx': [297]},\n",
       " 226: {'word': 'kg', 'token_idx': [298]},\n",
       " 227: {'word': ')', 'token_idx': [299]},\n",
       " 228: {'word': 'or', 'token_idx': [300]},\n",
       " 229: {'word': 'SM', 'token_idx': [301]},\n",
       " 230: {'word': '-', 'token_idx': [302]},\n",
       " 231: {'word': '5887', 'token_idx': [303, 304]},\n",
       " 232: {'word': '(', 'token_idx': [305]},\n",
       " 233: {'word': '2', 'token_idx': [306]},\n",
       " 234: {'word': '.', 'token_idx': [307]},\n",
       " 235: {'word': '5', 'token_idx': [308]},\n",
       " 236: {'word': 'mg', 'token_idx': [309]},\n",
       " 237: {'word': '/', 'token_idx': [310]},\n",
       " 238: {'word': 'kg', 'token_idx': [311]},\n",
       " 239: {'word': ')', 'token_idx': [312]},\n",
       " 240: {'word': 'once', 'token_idx': [313, 314]},\n",
       " 241: {'word': 'every', 'token_idx': [315]},\n",
       " 242: {'word': '3', 'token_idx': [316]},\n",
       " 243: {'word': 'weeks', 'token_idx': [317]},\n",
       " 244: {'word': '.', 'token_idx': [318]},\n",
       " 245: {'word': 'The', 'token_idx': [319]},\n",
       " 246: {'word': 'low', 'token_idx': [320]},\n",
       " 247: {'word': '-', 'token_idx': [321]},\n",
       " 248: {'word': 'grade', 'token_idx': [322]},\n",
       " 249: {'word': 'cardiotoxic', 'token_idx': [323, 324]},\n",
       " 250: {'word': 'changes', 'token_idx': [325]},\n",
       " 251: {'word': 'were', 'token_idx': [326]},\n",
       " 252: {'word': 'enhanced', 'token_idx': [327]},\n",
       " 253: {'word': 'by', 'token_idx': [328]},\n",
       " 254: {'word': 'the', 'token_idx': [329]},\n",
       " 255: {'word': 'additional', 'token_idx': [330, 331]},\n",
       " 256: {'word': 'doxorubicin', 'token_idx': [332, 333, 334]},\n",
       " 257: {'word': 'treatment', 'token_idx': [335]},\n",
       " 258: {'word': '.', 'token_idx': [336]},\n",
       " 259: {'word': 'On', 'token_idx': [337]},\n",
       " 260: {'word': 'the', 'token_idx': [338]},\n",
       " 261: {'word': 'contrary', 'token_idx': [339, 340]},\n",
       " 262: {'word': ',', 'token_idx': [341]},\n",
       " 263: {'word': 'the', 'token_idx': [342]},\n",
       " 264: {'word': 'SM', 'token_idx': [343]},\n",
       " 265: {'word': '-', 'token_idx': [344]},\n",
       " 266: {'word': '5887', 'token_idx': [345, 346]},\n",
       " 267: {'word': 'treatment', 'token_idx': [347]},\n",
       " 268: {'word': 'did', 'token_idx': [348]},\n",
       " 269: {'word': 'not', 'token_idx': [349]},\n",
       " 270: {'word': 'progress', 'token_idx': [350, 351, 352]},\n",
       " 271: {'word': 'the', 'token_idx': [353]},\n",
       " 272: {'word': 'grade', 'token_idx': [354]},\n",
       " 273: {'word': 'of', 'token_idx': [355]},\n",
       " 274: {'word': 'cardiomyopathy', 'token_idx': [356, 357, 358]},\n",
       " 275: {'word': '.', 'token_idx': [359]},\n",
       " 276: {'word': 'In', 'token_idx': [360]},\n",
       " 277: {'word': 'conclusion', 'token_idx': [361, 362]},\n",
       " 278: {'word': ',', 'token_idx': [363]},\n",
       " 279: {'word': 'SM', 'token_idx': [364]},\n",
       " 280: {'word': '-', 'token_idx': [365]},\n",
       " 281: {'word': '5887', 'token_idx': [366, 367]},\n",
       " 282: {'word': 'does', 'token_idx': [368, 369]},\n",
       " 283: {'word': 'not', 'token_idx': [370]},\n",
       " 284: {'word': 'have', 'token_idx': [371]},\n",
       " 285: {'word': 'any', 'token_idx': [372]},\n",
       " 286: {'word': 'potential', 'token_idx': [373]},\n",
       " 287: {'word': 'of', 'token_idx': [374]},\n",
       " 288: {'word': 'chronic', 'token_idx': [375]},\n",
       " 289: {'word': 'cardiotoxicity', 'token_idx': [376, 377]},\n",
       " 290: {'word': 'and', 'token_idx': [378]},\n",
       " 291: {'word': 'deteriorating', 'token_idx': [379, 380, 381]},\n",
       " 292: {'word': 'effect', 'token_idx': [382]},\n",
       " 293: {'word': 'on', 'token_idx': [383]},\n",
       " 294: {'word': 'doxorubicin', 'token_idx': [384, 385, 386]},\n",
       " 295: {'word': '-', 'token_idx': [387]},\n",
       " 296: {'word': 'induced', 'token_idx': [388]},\n",
       " 297: {'word': 'cardiotoxicity', 'token_idx': [389, 390]},\n",
       " 298: {'word': 'in', 'token_idx': [391]},\n",
       " 299: {'word': 'dogs', 'token_idx': [392, 393]},\n",
       " 300: {'word': '.', 'token_idx': [394]}}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_token_map[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "db86741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Valproic',\n",
       " 'acid',\n",
       " '(',\n",
       " 'VPA',\n",
       " ')',\n",
       " 'was',\n",
       " 'given',\n",
       " 'to',\n",
       " '24',\n",
       " 'epileptic',\n",
       " 'patients',\n",
       " 'who',\n",
       " 'were',\n",
       " 'already',\n",
       " 'being',\n",
       " 'treated',\n",
       " 'with',\n",
       " 'other',\n",
       " 'antiepileptic',\n",
       " 'drugs',\n",
       " '.',\n",
       " 'A',\n",
       " 'standardized',\n",
       " 'loading',\n",
       " 'dose',\n",
       " 'of',\n",
       " 'VPA',\n",
       " 'was',\n",
       " 'administered',\n",
       " ',',\n",
       " 'and',\n",
       " 'venous',\n",
       " 'blood',\n",
       " 'was',\n",
       " 'sampled',\n",
       " 'at',\n",
       " '0',\n",
       " ',',\n",
       " '1',\n",
       " ',',\n",
       " '2',\n",
       " ',',\n",
       " '3',\n",
       " ',',\n",
       " 'and',\n",
       " '4',\n",
       " 'hours',\n",
       " '.',\n",
       " 'Ammonia',\n",
       " '(',\n",
       " 'NH3',\n",
       " ')',\n",
       " 'was',\n",
       " 'higher',\n",
       " 'in',\n",
       " 'patients',\n",
       " 'who',\n",
       " ',',\n",
       " 'during',\n",
       " 'continuous',\n",
       " 'therapy',\n",
       " ',',\n",
       " 'complained',\n",
       " 'of',\n",
       " 'drowsiness',\n",
       " '(',\n",
       " '7',\n",
       " 'patients',\n",
       " ')',\n",
       " 'than',\n",
       " 'in',\n",
       " 'those',\n",
       " 'who',\n",
       " 'were',\n",
       " 'symptom',\n",
       " '-',\n",
       " 'free',\n",
       " '(',\n",
       " '17',\n",
       " 'patients',\n",
       " ')',\n",
       " ',',\n",
       " 'although',\n",
       " 'VPA',\n",
       " 'plasma',\n",
       " 'levels',\n",
       " 'were',\n",
       " 'similar',\n",
       " 'in',\n",
       " 'both',\n",
       " 'groups',\n",
       " '.',\n",
       " 'By',\n",
       " 'measuring',\n",
       " 'VPA',\n",
       " '-',\n",
       " 'induced',\n",
       " 'changes',\n",
       " 'of',\n",
       " 'blood',\n",
       " 'NH3',\n",
       " 'content',\n",
       " ',',\n",
       " 'it',\n",
       " 'may',\n",
       " 'be',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'patients',\n",
       " 'at',\n",
       " 'higher',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'obtundation',\n",
       " 'when',\n",
       " 'VPA',\n",
       " 'is',\n",
       " 'given',\n",
       " 'chronically',\n",
       " '.']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''\n",
    "Valproic acid (VPA) was given to 24 epileptic patients who were already being treated with other antiepileptic drugs. A standardized loading dose of VPA was administered, and venous blood was sampled at 0, 1, 2, 3, and 4 hours. Ammonia (NH3) was higher in patients who, during continuous therapy, complained of drowsiness (7 patients) than in those who were symptom-free (17 patients), although VPA plasma levels were similar in both groups. By measuring VPA-induced changes of blood NH3 content, it may be possible to identify patients at higher risk of obtundation when VPA is given chronically.\n",
    "'''\n",
    "s.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation})).split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9d32c",
   "metadata": {},
   "source": [
    "## 6. Comparing Against Old Results\n",
    "- Looks the same, so the current pipeline is the correct implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b448063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 4, ..., 0, 0, 0],\n",
       "       [0, 1, 3, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 4, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare against the old results inherited from the previous notebook\n",
    "preds  ## DO NOT RE-RUN THIS CELL.  THIS SIMPLY SHOWS THE ACTUAL RESULTS WHEN FEEDING IN THE DEFAULT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e90ae411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 512)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 500 samples, with each having 512 tokens (max token length):\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7f512f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the prediction numpy array: (512,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 4, 4, 4, 4, 4, 4, 0, 1, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 4, 4, 4, 4, 4, 0, 2, 4, 4, 0, 2, 4, 4, 4, 4, 0,\n",
       "       0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4,\n",
       "       4, 4, 4, 4, 0, 2, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0,\n",
       "       0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4,\n",
       "       4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 4, 0, 2, 4, 0, 0, 2,\n",
       "       4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's also look at the output for the first sample:\n",
    "print(f\"Shape of the prediction numpy array: {preds[0].shape}\")\n",
    "preds[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
