{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af66dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(task='cdr', data_dir='./dataset/cdr', transformer_type='bert', model_name_or_path='./RE_base', train_file='train_filter.data', dev_file='dev_filter.data', test_file='test_filter.data', save_path='out\\\\train_filter_bert_cdr_seed_BSCELoss_tree_0.3_0.5_66', load_path='', config_name='', tokenizer_name='', max_seq_length=1024, train_batch_size=12, test_batch_size=12, gradient_accumulation_steps=1, learning_rate=2e-05, adam_epsilon=1e-06, max_grad_norm=1.0, warmup_ratio=0.06, num_train_epochs=30, evaluation_steps=-1, seed=66, num_class=2, gnn='GCN', use_gcn='tree', dropout=0.5, loss='BSCELoss', s0=0.3, demo='false', unet_in_dim=3, unet_out_dim=256, down_dim=256, bert_lr=3e-05, max_height=64, rel2=0, save_result='', save_pubtator='./result/cdr/cdr_BSCELoss_tree_s0=0.3_dropout=0.5_66')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./RE_base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from model_bio import Model  ## Note: a module name change for model_utils was made to model_bio.py to avoid crashes with ConNER\n",
    "from scibert_utils import set_seed   ## Note: changed module name from utils to scibert_utils since ConNER has a similar module\n",
    "#from prepro_bio import read_bio\n",
    "#from save_result import Logger\n",
    "#from evaluation import train, evaluate\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", default=\"cdr\", type=str)\n",
    "parser.add_argument(\"--data_dir\", default=\"./dataset/cdr\", type=str)\n",
    "parser.add_argument(\"--transformer_type\", default=\"bert\", type=str)\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"bert-base-cased\", type=str)\n",
    "parser.add_argument(\"--train_file\", default=\"Train.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--dev_file\", default=\"Dev.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--test_file\", default=\"Test.BioC.JSON\", type=str)\n",
    "parser.add_argument(\"--save_path\", default=\"out\", type=str)\n",
    "parser.add_argument(\"--load_path\", default=\"\", type=str)\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--max_seq_length\", default=1024, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=4, type=int, help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--test_batch_size\", default=8, type=int, help=\"Batch size for testing.\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", default=1, type=int,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0.06, type=float, help=\"Warm up ratio for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=30.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--evaluation_steps\", default=-1, type=int, help=\"Number of training steps between evaluations.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--num_class\", type=int, default=97, help=\"Number of relation types in dataset.\")\n",
    "parser.add_argument('--gnn', type=str, default='GCN', help=\"GCN/GAT\")\n",
    "parser.add_argument('--use_gcn', type=str, default='tree', help=\"use gcn, both/mentions/tree/false\")\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help=\"0.0/0.2/0.5\")\n",
    "parser.add_argument('--loss', type=str, default='BSCELoss',\n",
    "                    help=\"use BSCELoss/BalancedLoss/ATLoss/AsymmetricLoss/APLLoss\")\n",
    "parser.add_argument('--s0', type=float, default=0.3)\n",
    "parser.add_argument(\"--demo\", type=str, default='false', help='use a few data to test. default true/false')\n",
    "parser.add_argument(\"--unet_in_dim\", type=int, default=3, help=\"unet_in_dim.\")\n",
    "parser.add_argument(\"--unet_out_dim\", type=int, default=256, help=\"unet_out_dim.\")\n",
    "parser.add_argument(\"--down_dim\", type=int, default=256, help=\"down_dim.\")\n",
    "parser.add_argument(\"--bert_lr\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--max_height\", type=int, default=64, help=\"max_height.\")\n",
    "parser.add_argument(\"--rel2\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--save_result\", type=str, default=\"\", help=\"save predict result.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "if args.task == 'cdr':\n",
    "    args.data_dir = './dataset/cdr'\n",
    "    args.train_file = 'train_filter.data'\n",
    "    args.dev_file = 'dev_filter.data'\n",
    "    args.test_file = 'test_filter.data'\n",
    "    args.model_name_or_path = './RE_base'\n",
    "    args.train_batch_size = 12\n",
    "    args.test_batch_size = 12\n",
    "    args.learning_rate = 2e-5\n",
    "    args.num_class = 2\n",
    "    args.num_train_epochs = 30\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "file_name = \"{}_{}_{}_seed_{}_{}_{}_{}_{}\".format(\n",
    "    args.train_file.split('.')[0],\n",
    "    args.transformer_type, args.data_dir.split('/')[-1],\n",
    "    args.loss, args.use_gcn, args.s0, args.dropout, str(args.seed), )\n",
    "args.save_path = os.path.join(args.save_path, file_name)\n",
    "args.save_pubtator = os.path.join('./result/' + args.task + '/' + args.task  + '_' + args.loss\n",
    "                                  + '_' + str(args.use_gcn) + '_s0=' + str(args.s0)\n",
    "                                  + '_dropout=' + str(args.dropout) + '_' + str(args.seed))\n",
    "#if args.load_path == \"\":\n",
    "#     sys.stdout = Logger(stream=sys.stdout, filename=args.save_pubtator + '_test.log')\n",
    "#read = read_bio\n",
    "print(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path, num_labels=args.num_class, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, )\n",
    "model = AutoModel.from_pretrained(\n",
    "    args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, )\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type\n",
    "set_seed(args)\n",
    "model = Model(args, config, model, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d587031",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"model.embeddings.position_ids\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_filter_bert_cdr_seed_BSCELoss_tree_03_05_66_best\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"model.embeddings.position_ids\". "
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"train_filter_bert_cdr_seed_BSCELoss_tree_03_05_66_best\"\n",
    "model.load_state_dict(torch.load(model_checkpoint, weights_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
