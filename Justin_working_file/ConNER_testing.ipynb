{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ce9c7e",
   "metadata": {},
   "source": [
    "# ConNER: Model Testing\n",
    "\n",
    "This notebook makes an initial attempt at deploying the ConNER model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07d4a3",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a284131",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flashtool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb82ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae36bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model definition related\n",
    "import bs4\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from transformers import BertPreTrainedModel,BertForTokenClassification, BertModel, RobertaModel, RobertaTokenizer, BertPreTrainedModel, RobertaConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from  torch.nn.utils.rnn  import pack_padded_sequence\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss, KLDivLoss\n",
    "\n",
    "from transformers import BertConfig, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf20ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eval related\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Remember to copy the \"data_utils.py\" file from ConNER's repo\n",
    "from data_utils import load_and_cache_examples, tag_to_id, get_chunks  \n",
    "from flashtool import Logger\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4881f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff019a9",
   "metadata": {},
   "source": [
    "## 2. Loading the model\n",
    "- First cell defines the model class (from ConNER's REPO)\n",
    "- Second cell loads the model checkpoint fine-tuned on BC5CDR\n",
    "- Third cell inspects the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5e296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
    "    \"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin\",\n",
    "    \"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin\",\n",
    "    \"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-pytorch_model.bin\",\n",
    "    \"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-pytorch_model.bin\",\n",
    "    \"roberta-base-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-openai-detector-pytorch_model.bin\",\n",
    "    \"roberta-large-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-openai-detector-pytorch_model.bin\",\n",
    "}\n",
    "\n",
    "class RobertaForTokenClassification_v2(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
    "            Labels for computing the token classification loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification loss.\n",
    "        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n",
    "            Classification scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = RobertaForTokenClassification.from_pretrained('roberta-base')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, scores = outputs[:2]\n",
    "    \"\"\"\n",
    "    config_class = RobertaConfig\n",
    "    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.classifier2 = nn.Linear(config.hidden_size*2, config.num_labels)\n",
    "        self.bilstm = nn.LSTM(config.hidden_size, config.hidden_size, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "        self.lambda1 = 1e-1\n",
    "        self.lambda2 = 1e-3\n",
    "        self.epsilon = 1e-8\n",
    "        self.threshold = 0.3\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        label_mask=None,\n",
    "        entity_ids=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        seq_length = input_ids.shape[1]\n",
    "        device = input_ids.device\n",
    "\n",
    "        final_embedding = outputs[0]\n",
    "        sequence_output = self.dropout(final_embedding)\n",
    "\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \"\"\" Bilstm for label refinement \"\"\"\n",
    "        if entity_ids is not None:\n",
    "            entity_ids = entity_ids[:,:,None]\n",
    "            bilstm_hidden = self.rand_init_hidden(batch_size)\n",
    "            fst_bilstm_hidden = bilstm_hidden[0].to(device)\n",
    "            bst_bilstm_hidden = bilstm_hidden[1].to(device)\n",
    "\n",
    "            lstm_out, lstm_hidden = self.bilstm(sequence_output, (fst_bilstm_hidden, bst_bilstm_hidden))\n",
    "            lstm_out = lstm_out.contiguous().view(-1, self.config.hidden_size*2)\n",
    "            d_lstm_out = self.dropout(lstm_out)\n",
    "            l_out = self.classifier2(d_lstm_out)\n",
    "            lstm_feats = l_out.contiguous().view(batch_size, seq_length, -1)\n",
    "\n",
    "            \"\"\" make label representation similar on biomedical entities (without regarding to context representation) \"\"\"\n",
    "            sft_logits = self.softmax(logits)\n",
    "            sft_feats = self.softmax(lstm_feats)\n",
    "            kl_logit_lstm = F.kl_div(sft_logits.log(), sft_feats, None, None, 'sum')\n",
    "            kl_lstm_logit = F.kl_div(sft_feats.log(), sft_logits, None, None, 'sum')\n",
    "            kl_distill = (kl_logit_lstm + kl_lstm_logit) / 2\n",
    "\n",
    "            \"\"\" update entities with lstm and mlp classifier \"\"\"\n",
    "            sft_feats = sft_feats * entity_ids # mask for only updated entities\n",
    "            \n",
    "            \"\"\" update through uncertainties \"\"\"\n",
    "            uncertainty = -torch.sum(sft_logits * torch.log(sft_logits + self.epsilon), dim=2)\n",
    "            ones = torch.ones(uncertainty.shape).to(device)\n",
    "            zeros = torch.zeros(uncertainty.shape).to(device)\n",
    "            uncertainty_mask = torch.where(uncertainty > self.threshold, ones, zeros)\n",
    "            uncertainty_mask = uncertainty_mask[:,:,None]\n",
    "            sft_feats = sft_feats * uncertainty_mask\n",
    "\n",
    "            logits = logits + sft_feats\n",
    "\n",
    "        outputs = (logits, final_embedding, ) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None or label_mask is not None:\n",
    "                active_loss = True\n",
    "                if attention_mask is not None:\n",
    "                    active_loss = attention_mask.view(-1) == 1\n",
    "                if label_mask is not None:\n",
    "                    active_loss = active_loss & label_mask.view(-1)\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "\n",
    "            if labels.shape == logits.shape:\n",
    "                loss_fct = KLDivLoss()\n",
    "                if attention_mask is not None or label_mask is not None:\n",
    "                    active_labels = labels.view(-1, self.num_labels)[active_loss]\n",
    "                    loss = loss_fct(active_logits, active_labels)\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                if attention_mask is not None or label_mask is not None:\n",
    "                    active_labels = labels.view(-1)[active_loss]\n",
    "                    loss = loss_fct(active_logits, active_labels)\n",
    "                else:\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "            if entity_ids is not None:\n",
    "                active_lstm_logits = sft_feats.view(-1, self.num_labels)[active_loss]\n",
    "                lstm_loss = loss_fct(active_lstm_logits, active_labels)\n",
    "                final_loss = loss + (self.lambda1) * lstm_loss + (self.lambda2) * kl_distill\n",
    "                outputs = (final_loss,) + outputs\n",
    "            else:\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, final_embedding, (hidden_states), (attentions)\n",
    "\n",
    "    def rand_init_hidden(self, batch_size,):\n",
    "        \"\"\"\n",
    "        random initialize hidden variable\n",
    "        \"\"\"\n",
    "        return Variable(torch.randn(2 * 2, batch_size, self.config.hidden_size)), Variable(torch.randn(2 * 2, batch_size, self.config.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5550226",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading model\n",
    "model_path = \"./ConNER\"\n",
    "\n",
    "## It appears the checkpoint is a Roberta-based model as loading it using BERT model yields an error.\n",
    "#test_model  = BERTForTokenClassification_v2.from_pretrained(model_path)\n",
    "test_model  = RobertaForTokenClassification_v2.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5637c922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification_v2(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50008, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  (classifier2): Linear(in_features=2048, out_features=5, bias=True)\n",
       "  (bilstm): LSTM(1024, 1024, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (softmax): Softmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Inspecting the model architecture\n",
    "test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee5ccf",
   "metadata": {},
   "source": [
    "## 3. Model Eval (Work in Progress)\n",
    "- First cell loads the eval function from ConNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30c7db2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66553061ff3e4fb49bfdca8d3019c84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JustinTo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6641732a8b384044aacd4acebf682504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b6a1a29a7042ac98c3b1327151b8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99dc3213f404fa68b21291c4c19f3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "498fbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels, hp_labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "        self.hp_labels = hp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dee004ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/bc5cdr/from_rawdata/doc_dev.json'\n",
    "guid_index = 1\n",
    "examples = []\n",
    "\n",
    "mode = 'doc_dev'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "        \n",
    "    for item in data:\n",
    "        words = item[\"str_words\"]\n",
    "        labels = item[\"tags\"]\n",
    "        if \"tags_hp\" in labels:\n",
    "            hp_labels = item[\"tags_hp\"]\n",
    "        else:\n",
    "            hp_labels = [None]*len(labels)\n",
    "        examples.append(InputExample(guid=\"%s-%d\".format(mode, guid_index), words=words, labels=labels, hp_labels=hp_labels))\n",
    "        guid_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a61cf39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_examples_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_examples_to_features\u001b[49m(\n\u001b[0;32m      2\u001b[0m             examples,\n\u001b[0;32m      3\u001b[0m             labels,\n\u001b[0;32m      4\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[0;32m      5\u001b[0m             tokenizer,\n\u001b[0;32m      6\u001b[0m             cls_token_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(args\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;66;03m# xlnet has a cls token at the end\u001b[39;00m\n\u001b[0;32m      8\u001b[0m             cls_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcls_token,\n\u001b[0;32m      9\u001b[0m             cls_token_segment_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     10\u001b[0m             sep_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msep_token,\n\u001b[0;32m     11\u001b[0m             sep_token_extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(args\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;66;03m# roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\u001b[39;00m\n\u001b[0;32m     13\u001b[0m             pad_on_left\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(args\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;66;03m# pad on the left for xlnet\u001b[39;00m\n\u001b[0;32m     15\u001b[0m             pad_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([tokenizer\u001b[38;5;241m.\u001b[39mpad_token])[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     16\u001b[0m             pad_token_segment_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     17\u001b[0m             pad_token_label_id\u001b[38;5;241m=\u001b[39mpad_token_label_id,\n\u001b[0;32m     18\u001b[0m             entity_name\u001b[38;5;241m=\u001b[39mentity_name,\n\u001b[0;32m     19\u001b[0m         )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'convert_examples_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "features = convert_examples_to_features(\n",
    "            examples,\n",
    "            labels,\n",
    "            args.max_seq_length,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=bool(args.model_type in [\"roberta\"]),\n",
    "            # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "            pad_on_left=bool(args.model_type in [\"xlnet\"]),\n",
    "            # pad on the left for xlnet\n",
    "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
    "            pad_token_label_id=pad_token_label_id,\n",
    "            entity_name=entity_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "710ce1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fd90a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['str_words', 'tags'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4f8644a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 565, 4063, 26781, 808, 2], [0, 6486, 548, 2], [0, 4950, 7150, 12257, 2], [0, 463, 2], [0, 462, 3432, 4031, 2], [0, 23612, 877, 2], [0, 90, 46513, 2], [0, 179, 2], [0, 102, 2], [0, 4651, 5400, 2], [0, 9433, 927, 2], [0, 4, 2], [0, 250, 2], [0, 4651, 5400, 2], [0, 5632, 2], [0, 31065, 2], [0, 90, 4063, 26781, 808, 2], [0, 4950, 7150, 12257, 2], [0, 6, 2], [0, 415, 13700, 2], [0, 4825, 12158, 2], [0, 6, 2], [0, 37519, 990, 2088, 2], [0, 12690, 2], [0, 37694, 2407, 2], [0, 6, 2], [0, 463, 2], [0, 102, 2], [0, 3530, 2], [0, 12778, 783, 2], [0, 462, 3432, 4031, 2], [0, 4483, 2], [0, 354, 2], [0, 30343, 2], [0, 4, 2], [0, 713, 2], [0, 354, 2], [0, 627, 2], [0, 9502, 2], [0, 23846, 2], [0, 560, 2], [0, 25153, 11416, 2], [0, 397, 42076, 2], [0, 90, 4063, 26781, 808, 2], [0, 4950, 7150, 12257, 2], [0, 463, 2], [0, 415, 13700, 2], [0, 4825, 12158, 2], [0, 6, 2], [0, 463, 2], [0, 627, 2], [0, 1225, 212, 2], [0, 30343, 2], [0, 23846, 2], [0, 5632, 2], [0, 6940, 9504, 2], [0, 417, 1496, 3175, 2], [0, 31636, 2], [0, 9433, 3277, 2], [0, 18793, 7878, 2], [0, 560, 2], [0, 462, 3432, 4031, 2], [0, 11828, 12363, 2], [0, 179, 2], [0, 627, 2], [0, 9502, 2], [0, 4328, 38417, 2], [0, 1116, 2], [0, 642, 34468, 2], [0, 4, 2], [0, 104, 29262, 2], [0, 12, 2], [0, 9983, 2], [0, 13566, 2], [0, 1116, 2], [0, 29902, 2], [0, 9433, 3277, 2], [0, 12186, 2], [0, 90, 4063, 26781, 808, 2], [0, 6486, 548, 2], [0, 24701, 18224, 1757, 2], [0, 4, 2], [0, 574, 3432, 4031, 2], [0, 23612, 877, 2], [0, 12488, 2], [0, 1610, 2], [0, 102, 2], [0, 31192, 2], [0, 179, 2], [0, 627, 2], [0, 32313, 2], [0, 3976, 17444, 2], [0, 1116, 2], [0, 37519, 225, 8632, 2], [0, 12690, 2], [0, 417, 1496, 3175, 2], [0, 14746, 2], [0, 90, 12578, 2], [0, 37460, 2], [0, 23099, 2], [0, 642, 34468, 2], [0, 4, 2], [0, 243, 2], [0, 19726, 2], [0, 3245, 9764, 2], [0, 858, 7367, 12376, 636, 2], [0, 17272, 21791, 2], [0, 6, 2], [0, 4469, 260, 13310, 2], [0, 6, 2], [0, 463, 2], [0, 6940, 9504, 2], [0, 6166, 298, 20436, 24238, 2], [0, 14746, 2], [0, 10998, 28817, 2], [0, 13718, 368, 2], [0, 560, 2], [0, 10273, 26230, 2], [0, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data0 = tokenizer(data[0]['str_words'])\n",
    "tokenized_data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "197573de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 565, 4063, 26781, 808, 2],\n",
       "  [0, 6486, 548, 2],\n",
       "  [0, 4950, 7150, 12257, 2],\n",
       "  [0, 463, 2],\n",
       "  [0, 462, 3432, 4031, 2],\n",
       "  [0, 23612, 877, 2],\n",
       "  [0, 90, 46513, 2],\n",
       "  [0, 179, 2],\n",
       "  [0, 102, 2],\n",
       "  [0, 4651, 5400, 2],\n",
       "  [0, 9433, 927, 2],\n",
       "  [0, 4, 2],\n",
       "  [0, 250, 2],\n",
       "  [0, 4651, 5400, 2],\n",
       "  [0, 5632, 2],\n",
       "  [0, 31065, 2],\n",
       "  [0, 90, 4063, 26781, 808, 2],\n",
       "  [0, 4950, 7150, 12257, 2],\n",
       "  [0, 6, 2],\n",
       "  [0, 415, 13700, 2],\n",
       "  [0, 4825, 12158, 2],\n",
       "  [0, 6, 2],\n",
       "  [0, 37519, 990, 2088, 2],\n",
       "  [0, 12690, 2],\n",
       "  [0, 37694, 2407, 2],\n",
       "  [0, 6, 2],\n",
       "  [0, 463, 2],\n",
       "  [0, 102, 2],\n",
       "  [0, 3530, 2],\n",
       "  [0, 12778, 783, 2],\n",
       "  [0, 462, 3432, 4031, 2],\n",
       "  [0, 4483, 2],\n",
       "  [0, 354, 2],\n",
       "  [0, 30343, 2],\n",
       "  [0, 4, 2],\n",
       "  [0, 713, 2],\n",
       "  [0, 354, 2],\n",
       "  [0, 627, 2],\n",
       "  [0, 9502, 2],\n",
       "  [0, 23846, 2],\n",
       "  [0, 560, 2],\n",
       "  [0, 25153, 11416, 2],\n",
       "  [0, 397, 42076, 2],\n",
       "  [0, 90, 4063, 26781, 808, 2],\n",
       "  [0, 4950, 7150, 12257, 2],\n",
       "  [0, 463, 2],\n",
       "  [0, 415, 13700, 2],\n",
       "  [0, 4825, 12158, 2],\n",
       "  [0, 6, 2],\n",
       "  [0, 463, 2],\n",
       "  [0, 627, 2],\n",
       "  [0, 1225, 212, 2],\n",
       "  [0, 30343, 2],\n",
       "  [0, 23846, 2],\n",
       "  [0, 5632, 2],\n",
       "  [0, 6940, 9504, 2],\n",
       "  [0, 417, 1496, 3175, 2],\n",
       "  [0, 31636, 2],\n",
       "  [0, 9433, 3277, 2],\n",
       "  [0, 18793, 7878, 2],\n",
       "  [0, 560, 2],\n",
       "  [0, 462, 3432, 4031, 2],\n",
       "  [0, 11828, 12363, 2],\n",
       "  [0, 179, 2],\n",
       "  [0, 627, 2],\n",
       "  [0, 9502, 2],\n",
       "  [0, 4328, 38417, 2],\n",
       "  [0, 1116, 2],\n",
       "  [0, 642, 34468, 2],\n",
       "  [0, 4, 2],\n",
       "  [0, 104, 29262, 2],\n",
       "  [0, 12, 2],\n",
       "  [0, 9983, 2],\n",
       "  [0, 13566, 2],\n",
       "  [0, 1116, 2],\n",
       "  [0, 29902, 2],\n",
       "  [0, 9433, 3277, 2],\n",
       "  [0, 12186, 2],\n",
       "  [0, 90, 4063, 26781, 808, 2],\n",
       "  [0, 6486, 548, 2],\n",
       "  [0, 24701, 18224, 1757, 2],\n",
       "  [0, 4, 2],\n",
       "  [0, 574, 3432, 4031, 2],\n",
       "  [0, 23612, 877, 2],\n",
       "  [0, 12488, 2],\n",
       "  [0, 1610, 2],\n",
       "  [0, 102, 2],\n",
       "  [0, 31192, 2],\n",
       "  [0, 179, 2],\n",
       "  [0, 627, 2],\n",
       "  [0, 32313, 2],\n",
       "  [0, 3976, 17444, 2],\n",
       "  [0, 1116, 2],\n",
       "  [0, 37519, 225, 8632, 2],\n",
       "  [0, 12690, 2],\n",
       "  [0, 417, 1496, 3175, 2],\n",
       "  [0, 14746, 2],\n",
       "  [0, 90, 12578, 2],\n",
       "  [0, 37460, 2],\n",
       "  [0, 23099, 2],\n",
       "  [0, 642, 34468, 2],\n",
       "  [0, 4, 2],\n",
       "  [0, 243, 2],\n",
       "  [0, 19726, 2],\n",
       "  [0, 3245, 9764, 2],\n",
       "  [0, 858, 7367, 12376, 636, 2],\n",
       "  [0, 17272, 21791, 2],\n",
       "  [0, 6, 2],\n",
       "  [0, 4469, 260, 13310, 2],\n",
       "  [0, 6, 2],\n",
       "  [0, 463, 2],\n",
       "  [0, 6940, 9504, 2],\n",
       "  [0, 6166, 298, 20436, 24238, 2],\n",
       "  [0, 14746, 2],\n",
       "  [0, 10998, 28817, 2],\n",
       "  [0, 13718, 368, 2],\n",
       "  [0, 560, 2],\n",
       "  [0, 10273, 26230, 2],\n",
       "  [0, 4, 2]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1]],\n",
       " 'labels': [2,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input_ids\": tokenized_data0['input_ids'],\n",
    "          \"attention_mask\": tokenized_data0['attention_mask'],\n",
    "          \"labels\": data[0]['tags']}\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3c37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text0 = '''Tricuspid valve regurgitation and lithium carbonate toxicity in a newborn infant. A newborn with massive tricuspid regurgitation, atrial flutter, congestive heart failure, and a high serum lithium level is described. This is the first patient to initially manifest tricuspid regurgitation and atrial flutter, and the 11th described patient with cardiac disease among infants exposed to lithium compounds in the first trimester of pregnancy. Sixty-three percent of these infants had tricuspid valve involvement. Lithium carbonate may be a factor in the increasing incidence of congenital heart disease when taken during early pregnancy. It also causes neurologic depression, cyanosis, and cardiac arrhythmia when consumed prior to delivery.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb670a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 565,\n",
       " 4063,\n",
       " 26781,\n",
       " 808,\n",
       " 24423,\n",
       " 6701,\n",
       " 7150,\n",
       " 12257,\n",
       " 8,\n",
       " 16904,\n",
       " 4363,\n",
       " 877,\n",
       " 35260,\n",
       " 11,\n",
       " 10,\n",
       " 14354,\n",
       " 12099,\n",
       " 4,\n",
       " 83,\n",
       " 14354,\n",
       " 19,\n",
       " 2232,\n",
       " 2664,\n",
       " 636,\n",
       " 26781,\n",
       " 808,\n",
       " 6701,\n",
       " 7150,\n",
       " 12257,\n",
       " 6,\n",
       " 23,\n",
       " 13700,\n",
       " 2342,\n",
       " 12158,\n",
       " 6,\n",
       " 29367,\n",
       " 2088,\n",
       " 1144,\n",
       " 2988,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 239,\n",
       " 38994,\n",
       " 16904,\n",
       " 672,\n",
       " 16,\n",
       " 1602,\n",
       " 4,\n",
       " 152,\n",
       " 16,\n",
       " 5,\n",
       " 78,\n",
       " 3186,\n",
       " 7,\n",
       " 3225,\n",
       " 19318,\n",
       " 2664,\n",
       " 636,\n",
       " 26781,\n",
       " 808,\n",
       " 6701,\n",
       " 7150,\n",
       " 12257,\n",
       " 8,\n",
       " 23,\n",
       " 13700,\n",
       " 2342,\n",
       " 12158,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 365,\n",
       " 212,\n",
       " 1602,\n",
       " 3186,\n",
       " 19,\n",
       " 17301,\n",
       " 2199,\n",
       " 566,\n",
       " 19964,\n",
       " 4924,\n",
       " 7,\n",
       " 16904,\n",
       " 18291,\n",
       " 11,\n",
       " 5,\n",
       " 78,\n",
       " 2664,\n",
       " 38417,\n",
       " 9,\n",
       " 6690,\n",
       " 4,\n",
       " 208,\n",
       " 29262,\n",
       " 12,\n",
       " 9983,\n",
       " 135,\n",
       " 9,\n",
       " 209,\n",
       " 19964,\n",
       " 56,\n",
       " 2664,\n",
       " 636,\n",
       " 26781,\n",
       " 808,\n",
       " 24423,\n",
       " 5292,\n",
       " 4,\n",
       " 26311,\n",
       " 4031,\n",
       " 4363,\n",
       " 877,\n",
       " 189,\n",
       " 28,\n",
       " 10,\n",
       " 3724,\n",
       " 11,\n",
       " 5,\n",
       " 2284,\n",
       " 24971,\n",
       " 9,\n",
       " 36764,\n",
       " 8632,\n",
       " 1144,\n",
       " 2199,\n",
       " 77,\n",
       " 551,\n",
       " 148,\n",
       " 419,\n",
       " 6690,\n",
       " 4,\n",
       " 85,\n",
       " 67,\n",
       " 4685,\n",
       " 31649,\n",
       " 636,\n",
       " 6943,\n",
       " 6,\n",
       " 39493,\n",
       " 13310,\n",
       " 6,\n",
       " 8,\n",
       " 17301,\n",
       " 25743,\n",
       " 298,\n",
       " 20436,\n",
       " 24238,\n",
       " 77,\n",
       " 13056,\n",
       " 2052,\n",
       " 7,\n",
       " 2996,\n",
       " 4,\n",
       " 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text0)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f8d607a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m test_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 2\u001b[0m test_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mRobertaForTokenClassification_v2.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, label_mask, entity_ids)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     59\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     entity_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ):\n\u001b[1;32m---> 70\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     80\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:794\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 794\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "test_model.eval()\n",
    "test_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffab8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, labels, pad_token_label_id, best, mode, entity_name, prefix=\"\", verbose=True):\n",
    "    \n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode, entity_name=entity_name)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    #if args.n_gpu > 1:\n",
    "    #    model = torch.nn.DataParallel(model)\n",
    "    #model.to(args.device)\n",
    "\n",
    "    logger.info(\"***** Running evaluation %s *****\", prefix)\n",
    "    if verbose:\n",
    "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                tmp_eval_loss = tmp_eval_loss.mean()\n",
    "\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    out_id_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_id_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "                out_id_list[i].append(out_label_ids[i][j])\n",
    "                preds_id_list[i].append(preds[i][j])\n",
    "            \n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0. # i variables\n",
    "    for ground_truth_id,predicted_id in zip(out_id_list,preds_id_list):\n",
    "        # We use the get chunks function defined above to get the true chunks\n",
    "        # and the predicted chunks from true labels and predicted labels respectively\n",
    "        data_dir = args.eval_dir\n",
    "        lab_chunks      = set(get_chunks(ground_truth_id, tag_to_id(data_dir)))\n",
    "        lab_pred_chunks = set(get_chunks(predicted_id, tag_to_id(data_dir)))\n",
    "\n",
    "        # Updating the i variables\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "\n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "\n",
    "    is_updated = False\n",
    "    if new_F > best[-1]:\n",
    "        best = [p, r, new_F]\n",
    "        is_updated = True\n",
    "\n",
    "    results = {\n",
    "       \"loss\": eval_loss,\n",
    "       \"precision\": p,\n",
    "       \"recall\": r,\n",
    "       \"f1\": new_F,\n",
    "       \"best_precision\": best[0],\n",
    "       \"best_recall\":best[1],\n",
    "       \"best_f1\": best[-1]\n",
    "    }\n",
    "\n",
    "    logger.info(\"***** Eval results %s *****\", prefix)\n",
    "    for key in sorted(results.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(results[key]))\n",
    "\n",
    "    return results, preds_list, best, is_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc81dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train_dir='./data/bc5cdr/from_rawdata', eval_dir='./data/bc5cdr/from_rawdata', model_type='roberta', model_name_or_path='./ConNER', output_dir='./output', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, adam_beta1=0.9, adam_beta2=0.98, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=10000, save_steps=10000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', mt=0, mt_updatefreq=1, mt_class='kl', mt_lambda=1, mt_rampup=300, mt_alpha1=0.99, mt_alpha2=0.995, mt_beta=10, mt_avg='exponential', mt_loss_type='logits', vat=0, vat_eps=0.001, vat_lambda=1, vat_beta=1, vat_loss_type='logits', load_weak=False, remove_labels_from_weak=False, rep_train_against_weak=1, wandb_name=None, data_type='str', data_name=None)\n"
     ]
    }
   ],
   "source": [
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "entity_name='bc5cdr'\n",
    "\n",
    "eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode, entity_name=entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9385ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x140de2e3280>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494c521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be19df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8300d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3ea7fadd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m evaluate(args,\n\u001b[1;32m----> 2\u001b[0m          \u001b[43mmodel\u001b[49m,\n\u001b[0;32m      3\u001b[0m          tokenizer,\n\u001b[0;32m      4\u001b[0m          labels,\n\u001b[0;32m      5\u001b[0m          pad_token_label_id,\n\u001b[0;32m      6\u001b[0m          best\u001b[38;5;241m=\u001b[39mbest_dev,\n\u001b[0;32m      7\u001b[0m          mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_dev\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m          entity_name\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_name,\n\u001b[0;32m      9\u001b[0m          prefix\u001b[38;5;241m=\u001b[39mglobal_step)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(args,\n",
    "         model,\n",
    "         tokenizer,\n",
    "         labels,\n",
    "         pad_token_label_id,\n",
    "         best=best_dev,\n",
    "         mode=\"doc_dev\",\n",
    "         entity_name=args.data_name,\n",
    "         prefix=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ecdf3",
   "metadata": {},
   "source": [
    "## XX. Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d95ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"roberta\": (RobertaTokenizer)\n",
    "}\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "parser.add_argument(\n",
    "        \"--train_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The input data dir. Should contain the training files for the CoNLL-2003 NER task.\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--eval_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The input data dir. Should contain the training files for the CoNLL-2003 NER task.\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True)\n",
    "parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True)\n",
    "parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "        \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
    "parser.add_argument(\n",
    "        \"--evaluate_during_training\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to run evaluation during training at each logging step.\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
    "    )\n",
    "\n",
    "parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\n",
    "        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--adam_beta1\", default=0.9, type=float, help=\"BETA1 for Adam optimizer.\")\n",
    "parser.add_argument(\"--adam_beta2\", default=0.98, type=float, help=\"BETA2 for Adam optimizer.\") # 0.999\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\n",
    "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "\n",
    "parser.add_argument(\"--logging_steps\", type=int, default=10000, help=\"Log every X updates steps.\")\n",
    "parser.add_argument(\"--save_steps\", type=int, default=10000, help=\"Save checkpoint every X updates steps.\")\n",
    "parser.add_argument(\n",
    "        \"--eval_all_checkpoints\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    "    )\n",
    "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\n",
    "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
    "    )\n",
    "parser.add_argument(\"--seed\", type=int, default=1, help=\"random seed for initialization\")\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "\n",
    "    # mean teacher\n",
    "parser.add_argument('--mt', type = int, default = 0, help = 'mean teacher.')\n",
    "parser.add_argument('--mt_updatefreq', type=int, default=1, help = 'mean teacher update frequency')\n",
    "parser.add_argument('--mt_class', type=str, default=\"kl\", help = 'mean teacher class, choices:[smart, prob, logit, kl(default), distill].')\n",
    "parser.add_argument('--mt_lambda', type=float, default=1, help= \"trade off parameter of the consistent loss.\")\n",
    "parser.add_argument('--mt_rampup', type=int, default=300, help=\"rampup iteration.\")\n",
    "parser.add_argument('--mt_alpha1', default=0.99, type=float, help=\"moving average parameter of mean teacher (for the exponential moving average).\")\n",
    "parser.add_argument('--mt_alpha2', default=0.995, type=float, help=\"moving average parameter of mean teacher (for the exponential moving average).\")\n",
    "parser.add_argument('--mt_beta', default=10, type=float, help=\"coefficient of mt_loss term.\")\n",
    "parser.add_argument('--mt_avg', default=\"exponential\", type=str, help=\"moving average method, choices:[exponentail(default), simple, double_ema].\")\n",
    "parser.add_argument('--mt_loss_type', default=\"logits\", type=str, help=\"subject to measure model difference, choices:[embeds, logits(default)].\")\n",
    "\n",
    "    # virtual adversarial training\n",
    "parser.add_argument('--vat', type = int, default = 0, help = 'virtual adversarial training.')\n",
    "parser.add_argument('--vat_eps', type = float, default = 1e-3, help = 'perturbation size for virtual adversarial training.')\n",
    "parser.add_argument('--vat_lambda', type = float, default = 1, help = 'trade off parameter for virtual adversarial training.')\n",
    "parser.add_argument('--vat_beta', type = float, default = 1, help = 'coefficient of the virtual adversarial training loss term.')\n",
    "parser.add_argument('--vat_loss_type', default=\"logits\", type=str, help=\"subject to measure model difference, choices = [embeds, logits(default)].\")\n",
    "\n",
    "    # Use data from weak.json\n",
    "parser.add_argument('--load_weak', action=\"store_true\", help = 'Load data from weak.json.')\n",
    "parser.add_argument('--remove_labels_from_weak', action=\"store_true\", help = 'Use data from weak.json, and remove their labels for semi-supervised learning')\n",
    "parser.add_argument('--rep_train_against_weak', type = int, default = 1, help = 'Upsampling training data again weak data. Default: 1')\n",
    "\n",
    "parser.add_argument('--wandb_name', type=str, default=None, help='Name of Wandb runs')\n",
    "parser.add_argument('--data_type', type=str, default=\"str\", help='Name of context level (e.g., sentence, document)')\n",
    "parser.add_argument('--data_name', type=str, default=None, help='Name of dataset')\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"--train_dir ./data/bc5cdr/from_rawdata --eval_dir ./data/bc5cdr/from_rawdata --model_type roberta --model_name_or_path ./ConNER --output_dir ./output\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "018d9b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(train_dir='./data/bc5cdr/from_rawdata', eval_dir='./data/bc5cdr/from_rawdata', model_type='roberta', model_name_or_path='./ConNER', output_dir='./output', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, adam_beta1=0.9, adam_beta2=0.98, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=10000, save_steps=10000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', mt=0, mt_updatefreq=1, mt_class='kl', mt_lambda=1, mt_rampup=300, mt_alpha1=0.99, mt_alpha2=0.995, mt_beta=10, mt_avg='exponential', mt_loss_type='logits', vat=0, vat_eps=0.001, vat_lambda=1, vat_beta=1, vat_loss_type='logits', load_weak=False, remove_labels_from_weak=False, rep_train_against_weak=1, wandb_name=None, data_type='str', data_name=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
